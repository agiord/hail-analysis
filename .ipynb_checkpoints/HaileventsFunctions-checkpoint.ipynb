{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee5d4aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "#import wradlib as wrl\n",
    "import pylab as pl\n",
    "from glob import glob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "try:\n",
    "    get_ipython().magic(\"matplotlib inline\")\n",
    "except:\n",
    "    pl.ion()\n",
    "import numpy as np\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "#shapefile: GIS vector data format (ESRI)\n",
    "import shapefile as shp  # Requires the pyshp package\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib import ticker\n",
    "import netCDF4 as nc4\n",
    "from datetime import date,timedelta\n",
    "import plotly.graph_objs as go\n",
    "import matplotlib.lines as mlines\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcd50fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Folder address containing data:\n",
    "fold = '/home/ciccuz/phd/KIT/hail_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debe2591",
   "metadata": {},
   "source": [
    "## IN THIS NOTEBOOK:\n",
    "\n",
    "- Functions for reading datasets\n",
    "- Functions for plotting\n",
    "- Functions for getting ESWD-based SPHERA reanalysis distributions\n",
    "- Functions for OT-filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6283fcb6",
   "metadata": {},
   "source": [
    "### Functions for reading datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf952ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function for reading city list to plot in map\n",
    "\"\"\"\n",
    "\n",
    "def HF_cities(latmin,latmax,lonmin,lonmax):\n",
    "    #Read and store cities and their features to be used in the maps:\n",
    "    with open(fold +'hail4_punge/geodata/cities/cities15000.txt') as file:\n",
    "        reader = csv.reader(file, delimiter='\\t') #, quoting=csv.QUOTE_NONNUMERIC)\n",
    "        next(reader);\n",
    "        clon=[]; clat=[]; csize=[];cname=[]; ckind=[]\n",
    "        for row in reader:\n",
    "            #store lon, lat, size (?? in km2 or m2?), name and kind (?? categorization?) of the cities:\n",
    "            clon.append(float(row[5]))\n",
    "            clat.append(float(row[4]))\n",
    "            csize.append(float(row[14]))\n",
    "            cname.append(row[1]) #.decode('utf-8') )\n",
    "            ckind.append(row[7]) #.decode('utf-8'))\n",
    "\n",
    "    #Select specific cities:\n",
    "    sellist=['Munich','Milano','Bologna','Parma','Piacenza','Genova','Genoa','Torino','Turin','Venezia','Pescara',\n",
    "             'Napoli','Bari','Roma',u'N\\xfcrnberg','Stuttgart','Prague','Dresden','Leipzig','Erfurt','Magdeburg',\n",
    "             'Berlin','Rostock','Hamburg','Hanover','Kiel','Innsbruck','Salzburg','Graz','Trento','Trient','Asti',\n",
    "            'Geneve','Genf']\n",
    "    clonsel=[clon[item] for item in range(len(clon)) if cname[item] in sellist and clat[item]>latmin and clat[item]<latmax and clon[item]>lonmin and clon[item]<lonmax];\n",
    "    clatsel=[clat[item] for item in range(len(clon)) if cname[item] in sellist and clat[item]>latmin and clat[item]<latmax and clon[item]>lonmin and clon[item]<lonmax];\n",
    "    csizesel=[csize[item] for item in range(len(clon)) if cname[item] in sellist and clat[item]>latmin and clat[item]<latmax and clon[item]>lonmin and clon[item]<lonmax];\n",
    "    cnamesel=[cname[item] for item in range(len(clon)) if cname[item] in sellist and clat[item] >latmin and clat[item]<latmax and clon[item]>lonmin and clon[item]<lonmax];\n",
    "    ckindsel=[ckind[item] for item in range(len(clon)) if cname[item] in sellist and clat[item] >latmin and clat[item]<latmax and clon[item]>lonmin and clon[item]<lonmax];\n",
    "\n",
    "    #Change city names with spelling errors or special characters:\n",
    "    for k in range(len(clonsel)):\n",
    "        if cnamesel[k] == u'N\\xfcrnberg':\n",
    "            cnamesel[k] ='Nuremberg';   \n",
    "\n",
    "        if cnamesel[k] == 'Hannover':\n",
    "            cnamesel[k]='Hanover';\n",
    "            \n",
    "    return clonsel, clatsel, csizesel, cnamesel, ckindsel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b499d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function for reading ESWD hail report data\n",
    "\"\"\"\n",
    "\n",
    "def HF_eswd_df(df_eswd,year_u,mon_u,day_u,hhmin,hhmax,latmin,latmax,lonmin,lonmax):\n",
    "    eswd_df = pd.DataFrame(columns=['datetime','lat','lon','size','QC_level'])\n",
    "    eswd_df['datetime'] = pd.to_datetime(df_eswd['TIME_EVENT'])\n",
    "    eswd_df['lat'] = df_eswd['LATITUDE']\n",
    "    eswd_df['lon'] = df_eswd['LONGITUDE']\n",
    "    eswd_df['size'] = df_eswd['MAX_HAIL_DIAMETER']\n",
    "    eswd_df['QC_level'] = df_eswd['QC_LEVEL']\n",
    "\n",
    "    eswd_df = eswd_df.sort_values(by='datetime').reset_index(drop=True)\n",
    "\n",
    "    #try:retain only data in the selected day and year and within hhmin and hhmax, as well as within latmin,latmax and lonmin,lonmax\n",
    "    eswd_df_years = eswd_df.datetime.apply(lambda x: x.year)\n",
    "    eswd_df_months = eswd_df.datetime.apply(lambda x: x.month)\n",
    "    eswd_df_days = eswd_df.datetime.apply(lambda x: x.day)\n",
    "    eswd_df_hours = eswd_df.datetime.apply(lambda x: x.hour)\n",
    "\n",
    "    eswd_df_sel = eswd_df.loc[(eswd_df_years == year_u) & (eswd_df_months == mon_u) & \n",
    "                              (eswd_df_days == day_u) & (eswd_df_hours >= hhmin) & (eswd_df_hours < hhmax)]\n",
    "    eswd_df_sel = eswd_df_sel.loc[(eswd_df_sel.lat >= latmin) & (eswd_df_sel.lat <= latmax)]\n",
    "    eswd_ev_sel = eswd_df_sel.loc[(eswd_df_sel.lon >= lonmin) & (eswd_df_sel.lon <= lonmax)].reset_index(drop=True)\n",
    "    \n",
    "    return eswd_ev_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b7dc390",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function for reading UNIPOL insurance claims\n",
    "\"\"\"\n",
    "\n",
    "def HF_unipol_df(df_unipol,year_u,mon_u,day_u,hhmin,hhmax,latmin,latmax,lonmin,lonmax):\n",
    "\n",
    "    #retain only data within the day of the event\n",
    "    dfu_ev = pd.DataFrame()\n",
    "    dfu_ev = df_unipol.loc[(df_unipol.year == year_u) & (df_unipol.month == mon_u) & (df_unipol.day == day_u)].reset_index(drop=True)\n",
    "\n",
    "    if len(dfu_ev != 0):\n",
    "        #save date lat lon data in arrays\n",
    "        u_ev = pd.DataFrame(columns=['datetime','lon','lat'])\n",
    "        u_ev['datetime'] = pd.to_datetime({'year': dfu_ev['year'],'month':dfu_ev['month'],'day':dfu_ev['day'],'hour':dfu_ev['hour'],\n",
    "                       'minute':dfu_ev['min']})\n",
    "        u_ev['lon'] = np.array(dfu_ev.lon)\n",
    "        u_ev['lat'] = np.array(dfu_ev.lat)\n",
    "\n",
    "        u_ev = u_ev.sort_values(by='datetime').reset_index(drop=True)\n",
    "\n",
    "        #should add a condition on keeping only the hours around the event (>hhmin, <hhmax) but since Unipol date has a large\n",
    "        #uncertainty around the exact timing (+-3hours) for now I keep them all....\n",
    "\n",
    "        #try:retain only data within hhmin and hhmax, as well as within latmin,latmax and lonmin,lonmax\n",
    "        u_ev_hours = u_ev.datetime.apply(lambda x: x.hour)\n",
    "        u_ev_sel = u_ev.loc[(u_ev_hours >= hhmin) & (u_ev_hours < hhmax)]\n",
    "        u_ev_sel = u_ev_sel.loc[(u_ev_sel.lat >= latmin) & (u_ev_sel.lat <= latmax)]\n",
    "        u_ev_sel = u_ev_sel.loc[(u_ev_sel.lon >= lonmin) & (u_ev_sel.lon <= lonmax)].reset_index(drop=True)\n",
    "        \n",
    "        u_ev_sel = gpd.GeoDataFrame(u_ev_sel, geometry=gpd.points_from_xy(u_ev_sel.lon, u_ev_sel.lat)) \n",
    "\n",
    "    else:\n",
    "        u_ev_sel = gpd.GeoDataFrame()\n",
    "    \n",
    "    return u_ev_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbbb2633",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function for reading EUCLID lightning data\n",
    "\"\"\"\n",
    "\n",
    "def HF_euclid_df(df_lg,year_u,mon_u,day_u,hhmin,hhmax,latmin,latmax,lonmin,lonmax):\n",
    "\n",
    "    df_lg.datetime = pd.to_datetime(df_lg.datetime)\n",
    "    \n",
    "    df_lg_months = df_lg.datetime.apply(lambda x: x.month)\n",
    "    df_lg_days = df_lg.datetime.apply(lambda x: x.day)\n",
    "    df_lg_hours = df_lg.datetime.apply(lambda x: x.hour)\n",
    "    \n",
    "    #select lightning data for month, day, and hours of the event, as well as lat lon\n",
    "    df_lg_sel = df_lg.loc[(df_lg_months == mon_u) & (df_lg_months == mon_u) &\n",
    "                          (df_lg_days == day_u) & (df_lg_days == day_u) &\n",
    "                          (df_lg_hours >= hhmin) & (df_lg_hours < hhmax)]\n",
    "    df_lg_sel = df_lg_sel.loc[(df_lg_sel.lat >= latmin) & (df_lg_sel.lat <= latmax)]\n",
    "    df_lg_sel = df_lg_sel.loc[(df_lg_sel.lon >= lonmin) & (df_lg_sel.lon <= lonmax)].reset_index(drop=True)\n",
    "    \n",
    "    return df_lg_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9ab532c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function for reading raw LAMPINET lightning data and calculate sum over day\n",
    "\"\"\"\n",
    "\n",
    "def HF_Lampinet_raw(year_u, mon_u, day_u, Nlig_min):\n",
    "    \n",
    "    #Read lampinet data and couple box number to coordinate locations:\n",
    "\n",
    "    #every file is related to one day, data for every box every 15 minutes -> tot: (10km grid) 16899 box * (24*15) steps\n",
    "    df_lamp = pd.read_csv(fold + 'data/lampinet/lampinet_2016-18_10km/lightning_' +\n",
    "                          f'{year_u}'+\"{:02d}\".format(mon_u)+\"{:02d}\".format(day_u)+'_grid_10.csv', sep=',', header=None)\n",
    "    df_lamp.columns = ['datetime','box_id','N_lightnings','Max_abs_intensity_kA'] \n",
    "    df_lamp['datetime'] = pd.to_datetime(df_lamp['datetime'], format='%Y%m%d%H%M')\n",
    "    \n",
    "    #sum all N_lightnings in a box for the entire day:\n",
    "    df_lamp_daysum = pd.DataFrame(columns=['box_id','N_lightnings'])\n",
    "\n",
    "    df_lamp_daysum['box_id'] = np.arange(1,16901,1)\n",
    "\n",
    "    for box in np.arange(1,16901,1):\n",
    "        df_lamp_daysum['N_lightnings'].loc[df_lamp_daysum['box_id'] == box] = sum(df_lamp.loc[df_lamp['box_id'] == box]['N_lightnings'])\n",
    "\n",
    "    #color-code data related to lightnings info: CRITERION -> at least Nlig_min lightning per box\n",
    "    df_lamp_daysum_filt = df_lamp_daysum.loc[df_lamp_daysum.N_lightnings > Nlig_min].N_lightnings.reset_index()\n",
    "    df_lamp_daysum_filt = df_lamp_daysum_filt.rename(columns={'index' : 'box_id'})\n",
    "    \n",
    "    return df_lamp_daysum_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d077fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function for reading raw LAMPINET LJ index for a specific hour of the day\n",
    "\"\"\"\n",
    "\n",
    "def HF_Lampinet_LJhourly(year_u, mon_u, day_u, lj_hour):\n",
    "\n",
    "    #Read lampinet data and couple box number to coordinate locations:\n",
    "\n",
    "    #every file is related to one day, data for every box every 15 minutes \n",
    "    df_lamp = pd.read_csv(fold + 'data/lampinet/lampinetIndex_2015-2020_20km/hail_' +\n",
    "                          f'{year_u}'+\"{:02d}\".format(mon_u)+\"{:02d}\".format(day_u)+'_grid_5_min1_jump20.csv',\n",
    "                          sep=r\"\\s+\",header=None)\n",
    "    df_lamp.columns = ['datetime','box_id','lon','lat','ind'] \n",
    "    df_lamp['datetime'] = pd.to_datetime(df_lamp['datetime'], format='%Y%m%d%H%M')\n",
    "    \n",
    "    #SELECT ONLY CERTAIN HOUR OF THE DAY TO RETAIN LJ DATA FOR PLOTTING\n",
    "    df_lamp_sel = df_lamp.loc[df_lamp.datetime.dt.hour == lj_hour]\n",
    "    df_lamp_sel = df_lamp_sel.reset_index(drop=True)\n",
    "    \n",
    "    return df_lamp_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "268cc40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function for reading OT data\n",
    "\"\"\"\n",
    "\n",
    "def HF_OTdata(day,hhmin,hhmax,otpref,otdir,othstart):\n",
    "    \n",
    "    ds=str(day)\n",
    "    \n",
    "    #names for the OT data files used to read them\n",
    "    otsep='_' \n",
    "    #othstart = \"0000\"\n",
    "    #otdir=fold + f'data/OT_SEVIRI_data/{day}/';\n",
    "    otposf='.nc';\n",
    "        \n",
    "    ncfile=otdir+otpref+ds+otsep+othstart+otposf;\n",
    "\n",
    "    #lat/lon coords to cover the whole SPHERA domain:\n",
    "    lonmin_S=6; lonmax_S=19; latmin_S=35; latmax_S=49\n",
    "    latplot_S=[latmin_S,latmax_S]; lonplot_S=[lonmin_S,lonmax_S]\n",
    "    \n",
    "    #Read 1 netcdf OT data\n",
    "    f = nc4.Dataset(ncfile,'r')\n",
    "    \n",
    "    #store OT probability (%) data and lat,lon of the first file (to create appropriate arrays to store data)\n",
    "    ref=f.variables['ot_probability'][:]    #OT data with 3 dimensions: 1 temporal (fictitious), and 2 spatial (lat,lon)\n",
    "    lons=f.variables['longitude'][:]        #lon data with 1 dimension: itself\n",
    "    lats=f.variables['latitude'][:]         #lat data \"\n",
    "    \n",
    "    #FILTER OUT OT DETECTIONS OUTSIDE SPHERA DOMAIN: extract spatial subset over SPHERA spatial domain only:\n",
    "    lons_S_ind = np.where((lons.data > lonplot_S[0]) & (lons.data < lonplot_S[1]))[0]\n",
    "    lats_S_ind = np.where((lats.data > latplot_S[0]) & (lats.data < latplot_S[1]))[0]\n",
    "    ref_S = f.variables['ot_probability'][:,lats_S_ind,lons_S_ind]\n",
    "\n",
    "    #substitute full lat/lon variables with subset data:\n",
    "    lons = lons[lons_S_ind]\n",
    "    lats = lats[lats_S_ind]\n",
    "    \n",
    "    #close the netcdf file\n",
    "    f.close()\n",
    "    \n",
    "    #create coordinate matrix from the two coordinate vectors of lats and lons:\n",
    "    longg, latg = np.meshgrid(lons, lats)  #longg: 980*1120 long arrays with the same range of longitudes covered; \n",
    "                                           #latg: 980*1120 long arrays having a unique value of lats in the same array\n",
    "                                           #and progressively increasing value at every array\n",
    "    #make copies of the matrix:\n",
    "    cpclon=1.*longg;cpclat=1.*latg;\n",
    "\n",
    "    #create matrices of 0s and 9999s with the same dimensions of the grid considered (for storing OT variables)\n",
    "    refmax=np.zeros([len(lats),len(lons)]); otpmax=refmax;\n",
    "    irbmin=refmax+9999; #print refmax.shape\n",
    "    dtmin=refmax+9999;\n",
    "    virmax=refmax;\n",
    "    otvmax=refmax;otimax=refmax;\n",
    "    zarr=np.zeros([1,len(lats),len(lons)]);\n",
    "    t_ot=[]\n",
    "    ot_timing=[]\n",
    "\n",
    "    #point to OT data files divided in 15 minutes intervals:\n",
    "    otfiles = sorted(glob(otdir+otpref+ds+otsep+\"*\"+otposf));\n",
    "    #print(len(otfiles))\n",
    "    \n",
    "    #Cycle to read and store the OT data and related variables from the set of separated files\n",
    "    for ncfile in otfiles[:]:\n",
    "        hh=int(ncfile[-7:-5])  #hour\n",
    "\n",
    "        #Condition on the hour of the day around the event [hhmin,hhmax]\n",
    "        if (hh>=hhmin)&(hh<hhmax):   # considering <=hhmax 2 hours are included:e.g. from hhmin(3.00pm)to \n",
    "                                      #hhmax(4.59pm)! -> consider <hhmax to retain only from 3.00 to 3.59!\n",
    "            vars=[];                  \n",
    "\n",
    "            f = nc4.Dataset(ncfile,'r');\n",
    "            for iv in f.variables.items():\n",
    "                vars.append(iv[0])\n",
    "\n",
    "            #initialize variables with zero arrays (zarr):\n",
    "            dtemp=zarr+9999;  #Delta temp: IR brightness temp (cloud top) - tropopause temperature (to quantify the vertical exstension, but not very robust, better result with difference between OT cloud and anvil cloud (not included here, new version of Bedka))\n",
    "            otv=zarr;         #OT rating visible (?)\n",
    "            oti=zarr;         #OT rating infrared (?)\n",
    "            vir=zarr;         #visible reflectance\n",
    "\n",
    "            #Parallax correction for OT: these coefficients are equal in every netcdf file\n",
    "            pclat=f.variables['parallax_correction_latitude'][:,lats_S_ind,lons_S_ind];\n",
    "            pclon=f.variables['parallax_correction_longitude'][:,lats_S_ind,lons_S_ind];\n",
    "\n",
    "            #OT probability\n",
    "            otp=f.variables['ot_probability'][:,lats_S_ind,lons_S_ind];\n",
    "            otp[(1-np.isfinite(otp[:]))>0]=0.;  #set to 0 all OTprob values exceeding 1 (i.e. 100% OT probability)\n",
    "\n",
    "            #store timing of max OT probability\n",
    "            t_ot.append(max((otp>.5)*f.variables['time']))\n",
    "    \n",
    "            ot_timing.append(f.variables['time'].date_time_stamp)\n",
    "        \n",
    "            #Conditions if variables are missing: set values to 0 or 9999\n",
    "            if np.any('visible_reflectance' in vars):\n",
    "                vir=f.variables['visible_reflectance'][:,lats_S_ind,lons_S_ind];\n",
    "                vir[(1-np.isfinite(vir[:]))>0]=0; \n",
    "            if np.any('ot_rating_ir' in vars):\n",
    "                oti=f.variables['ot_rating_ir'][:,lats_S_ind,lons_S_ind]; #oti=oti/256.;\n",
    "                oti[(1-np.isfinite(oti[:]))>0]=0; \n",
    "            if np.any('ot_rating_visible' in vars):\n",
    "                otv=f.variables['ot_rating_visible'][:,lats_S_ind,lons_S_ind]; #otv=otv.;\n",
    "                otv[(1-np.isfinite(otv[:]))>0]=0; \n",
    "            if np.any('tropopause_temperature' in vars):\n",
    "                tpt=f.variables['tropopause_temperature'][:,lats_S_ind,lons_S_ind];\n",
    "                tpt[(1-np.isfinite(tpt[:]))>0]=0;   \n",
    "            if np.any('ir_brightness_temperature' in vars):\n",
    "                irb=f.variables['ir_brightness_temperature'][:,lats_S_ind,lons_S_ind];\n",
    "                irb[(1-np.isfinite(irb[:]))>0]=9999; \n",
    "                dtemp=irb-tpt;\n",
    "\n",
    "            f.close()\n",
    "    \n",
    "            #Apply parallax correction:\n",
    "            selmax=irb[0,:,:]<irbmin;       #what is this needed for as irbmin is a matrix of 9999?\n",
    "            cpclat[selmax]=pclat[0,selmax]; # additive parallax correction coefficient in latitudes\n",
    "            cpclon[selmax]=pclon[0,selmax]; # additive parallax correction coefficient in longitudes\n",
    "            \n",
    "            otpmax=np.maximum(otpmax, otp[0,:,:]*(dtemp[0,:,:]<0))  #max OT prob. between the previous otpmax and OT \n",
    "                                                    #prob. matrix * delta temperature matrix with negative values\n",
    "            irbmin=np.minimum(irbmin, irb[0,:,:])  #minimum infrared brightness temperature (do retain the coldest pixel of the day)\n",
    "            dtmin=np.minimum(dtmin, dtemp[0,:,:])  #minimum delta temperature\n",
    "            otimax=np.maximum(otimax, oti[0,:,:])  #maximum OT rating infrared \n",
    "            otvmax=np.maximum(otvmax, otv[0,:,:])  #maximum OT rating visible\n",
    "            virmax=np.maximum(virmax, vir[0,:,:])  #maximum visible reflectance\n",
    "\n",
    "    #Add parallax correction coefficient to lat/lon coordinates\n",
    "    cpclat=cpclat+latg\n",
    "    cpclon=cpclon+longg\n",
    "    \n",
    "    otpmax[otpmax<5e-1]=np.nan  #condition on OT max prob.: set smaller values than 5e-1 to NaN\n",
    "    \"\"\"\n",
    "    Condition on dtmin: retain only values <20°C otherwise problems in plotting (WHy???)\n",
    "    \"\"\"\n",
    "    dtmin[dtmin>20]=np.nan    #dtmin>50\n",
    "    \n",
    "    return cpclat, cpclon, otpmax, dtmin, ot_timing    #bidimensional masked arrays as outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b81c9679",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function for reading OT data: store every hour (4 ncfiles) of the day separately in a dictionary\n",
    "\"\"\"\n",
    "\n",
    "def HF_OTdata_Hstore(day,hhmin,hhmax,otpref):\n",
    "    \n",
    "    ds=str(day)\n",
    "\n",
    "    #names for the OT data files used to read them\n",
    "    otsep='_' \n",
    "    othstart = \"0000\"\n",
    "    otdir=fold + f'data/OT_SEVIRI_data/{day}/';\n",
    "    otsep=otsep; otposf='.nc';\n",
    "\n",
    "    ncfile=otdir+otpref+ds+otsep+othstart+otposf;\n",
    "\n",
    "    #lat/lon coords to cover the whole SPHERA domain:\n",
    "    lonmin_S=6; lonmax_S=19; latmin_S=35; latmax_S=49\n",
    "    latplot_S=[latmin_S,latmax_S]; lonplot_S=[lonmin_S,lonmax_S]\n",
    "\n",
    "    #Read 1 netcdf OT data\n",
    "    f = nc4.Dataset(ncfile,'r')\n",
    "\n",
    "    #store OT probability (%) data and lat,lon of the first file (to create appropriate arrays to store data)\n",
    "    ref=f.variables['ot_probability'][:]    #OT data with 3 dimensions: 1 temporal (fictitious), and 2 spatial (lat,lon)\n",
    "    lons=f.variables['longitude'][:]        #lon data with 1 dimension: itself\n",
    "    lats=f.variables['latitude'][:]         #lat data \"\n",
    "\n",
    "    #FILTER OUT OT DETECTIONS OUTSIDE SPHERA DOMAIN: extract spatial subset over SPHERA spatial domain only:\n",
    "    lons_S_ind = np.where((lons.data > lonplot_S[0]) & (lons.data < lonplot_S[1]))[0]\n",
    "    lats_S_ind = np.where((lats.data > latplot_S[0]) & (lats.data < latplot_S[1]))[0]\n",
    "    ref_S = f.variables['ot_probability'][:,lats_S_ind,lons_S_ind]\n",
    "\n",
    "    #substitute full lat/lon variables with subset data:\n",
    "    lons = lons[lons_S_ind]\n",
    "    lats = lats[lats_S_ind]\n",
    "\n",
    "    #close the netcdf file\n",
    "    f.close()\n",
    "\n",
    "    #create coordinate matrix from the two coordinate vectors of lats and lons:\n",
    "    longg, latg = np.meshgrid(lons, lats)  #longg: 980*1120 long arrays with the same range of longitudes covered; \n",
    "                                           #latg: 980*1120 long arrays having a unique value of lats in the same array\n",
    "                                           #and progressively increasing value at every array\n",
    "    #dictionary to store separately OT data:\n",
    "    OT_dict_h = {}\n",
    "    OT_dict_h = dict.fromkeys(np.arange(0,24,1))\n",
    "    for hour in np.arange(0,24,1):\n",
    "        OT_dict_h[hour] = dict.fromkeys(['cpclat','cpclon','otpmax','ot_timing'])\n",
    "\n",
    "    #point to OT data files divided in 15 minutes intervals:\n",
    "    otfiles = sorted(glob(otdir+otpref+ds+otsep+\"*\"+otposf));\n",
    "\n",
    "    #cycle to store separately OT data for every hour (with hh comprised between hhmin and hhmax)\n",
    "    for hour in np.arange(hhmin,hhmax,1):\n",
    "\n",
    "        #make copies of the matrix:\n",
    "        cpclon=1.*longg;cpclat=1.*latg;\n",
    "\n",
    "        #create matrices of 0s and 9999s with the same dimensions of the grid considered (for storing OT variables)\n",
    "        refmax=np.zeros([len(lats),len(lons)]); otpmax=refmax;\n",
    "        irbmin=refmax+9999; #print refmax.shape\n",
    "        dtmin=refmax+9999;\n",
    "        virmax=refmax;\n",
    "        otvmax=refmax;otimax=refmax;\n",
    "        zarr=np.zeros([1,len(lats),len(lons)]);\n",
    "        t_ot=[]\n",
    "\n",
    "        #Cycle to read and store the OT data and related variables from the set of separated files\n",
    "        for ncfile in otfiles[:]:\n",
    "            hh=int(ncfile[-7:-5])  #hour\n",
    "\n",
    "            if hh == hour:\n",
    "\n",
    "                vars=[];                  \n",
    "\n",
    "                f = nc4.Dataset(ncfile,'r');\n",
    "                for iv in f.variables.items():\n",
    "                    vars.append(iv[0])\n",
    "\n",
    "                #initialize variables with zero arrays (zarr):\n",
    "                dtemp=zarr+9999;  #Delta temp: IR brightness temp (cloud top) - tropopause temperature (to quantify the vertical exstension, but not very robust, better result with difference between OT cloud and anvil cloud (not included here, new version of Bedka))\n",
    "                otv=zarr;         #OT rating visible (?)\n",
    "                oti=zarr;         #OT rating infrared (?)\n",
    "                vir=zarr;         #visible reflectance\n",
    "\n",
    "                #Parallax correction for OT: these coefficients are equal in every netcdf file\n",
    "                pclat=f.variables['parallax_correction_latitude'][:,lats_S_ind,lons_S_ind];\n",
    "                pclon=f.variables['parallax_correction_longitude'][:,lats_S_ind,lons_S_ind];\n",
    "\n",
    "                #OT probability\n",
    "                otp=f.variables['ot_probability'][:,lats_S_ind,lons_S_ind];\n",
    "                otp[(1-np.isfinite(otp[:]))>0]=0.;  #set to 0 all OTprob values exceeding 1 (i.e. 100% OT probability)\n",
    "\n",
    "                #store timing of max OT probability\n",
    "                t_ot.append(max((otp>.5)*f.variables['time']))\n",
    "\n",
    "                ot_timing = f.variables['time'].date_time_stamp\n",
    "\n",
    "                #Conditions if variables are missing: set values to 0 or 9999\n",
    "                if np.any('visible_reflectance' in vars):\n",
    "                    vir=f.variables['visible_reflectance'][:,lats_S_ind,lons_S_ind];\n",
    "                    vir[(1-np.isfinite(vir[:]))>0]=0; \n",
    "                if np.any('ot_rating_ir' in vars):\n",
    "                    oti=f.variables['ot_rating_ir'][:,lats_S_ind,lons_S_ind]; #oti=oti/256.;\n",
    "                    oti[(1-np.isfinite(oti[:]))>0]=0; \n",
    "                if np.any('ot_rating_visible' in vars):\n",
    "                    otv=f.variables['ot_rating_visible'][:,lats_S_ind,lons_S_ind]; #otv=otv.;\n",
    "                    otv[(1-np.isfinite(otv[:]))>0]=0; \n",
    "                if np.any('tropopause_temperature' in vars):\n",
    "                    tpt=f.variables['tropopause_temperature'][:,lats_S_ind,lons_S_ind];\n",
    "                    tpt[(1-np.isfinite(tpt[:]))>0]=0;   \n",
    "                if np.any('ir_brightness_temperature' in vars):\n",
    "                    irb=f.variables['ir_brightness_temperature'][:,lats_S_ind,lons_S_ind];\n",
    "                    irb[(1-np.isfinite(irb[:]))>0]=9999; \n",
    "                    dtemp=irb-tpt;\n",
    "\n",
    "                f.close()\n",
    "\n",
    "                #Apply parallax correction:\n",
    "                selmax=irb[0,:,:]<irbmin;       #what is this needed for as irbmin is a matrix of 9999?\n",
    "                cpclat[selmax]=pclat[0,selmax]; # additive parallax correction coefficient in latitudes\n",
    "                cpclon[selmax]=pclon[0,selmax]; # additive parallax correction coefficient in longitudes\n",
    "\n",
    "                otpmax=np.maximum(otpmax, otp[0,:,:]*(dtemp[0,:,:]<0))  #max OT prob. between the previous otpmax and OT \n",
    "                                                        #prob. matrix * delta temperature matrix with negative values\n",
    "                irbmin=np.minimum(irbmin, irb[0,:,:])  #minimum infrared brightness temperature (do retain the coldest pixel of the day)\n",
    "                dtmin=np.minimum(dtmin, dtemp[0,:,:])  #minimum delta temperature\n",
    "                otimax=np.maximum(otimax, oti[0,:,:])  #maximum OT rating infrared \n",
    "                otvmax=np.maximum(otvmax, otv[0,:,:])  #maximum OT rating visible\n",
    "                virmax=np.maximum(virmax, vir[0,:,:])  #maximum visible reflectance\n",
    "\n",
    "        #Add parallax correction coefficient to lat/lon coordinates\n",
    "        cpclat=cpclat+latg\n",
    "        cpclon=cpclon+longg\n",
    "\n",
    "        otpmax[otpmax<5e-1]=np.nan  #condition on OT max prob.: set smaller values than 5e-1 to NaN\n",
    "        \"\"\"\n",
    "        Condition on dtmin: retain only values <20°C otherwise problems in plotting (WHy???)\n",
    "        \"\"\"\n",
    "        dtmin[dtmin>20]=np.nan    #dtmin>50\n",
    "\n",
    "        #write variables in dictionary containing all OT detections seraparated for every hour:\n",
    "        OT_dict_h[hour] = {\"cpclat\":cpclat, \"cpclon\":cpclon, \"otpmax\":otpmax, \"ot_timing\":ot_timing}\n",
    "\n",
    "    return OT_dict_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc075967",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function for reading hourly gridded SPHERA convective proxies and aggregate them in 1 dataframe for the whole day\n",
    "\"\"\"\n",
    "\n",
    "def HF_reanProxies_read(dtime):\n",
    "    \n",
    "    sp_res = 10  #10km res grid, kept constant\n",
    "    \n",
    "    df_sp = pd.DataFrame(columns=[\"Data\",\"Macroarea\",\"%VV700\",\"AvvGeop500\",\"Kindex\",\"LI\",\"DeepShear\",\n",
    "                                  \"H0\",\"CAPE_MU\",\"CAPE_ML\"])\n",
    "\n",
    "    #read hourly dataframes and aggregate them to form a daily dataframe\n",
    "    for i in np.arange(0,24,1):\n",
    "\n",
    "        df_temp_SLI_K_AvvGeop_VV700 = pd.read_csv(fold + f'data/SPHERA/sphera_indices_grid{sp_res}km/SLI_K_AvvGeop_VV700/{dtime.year}/ind_'+\n",
    "                              f'{dtime.year}'+\"{:02d}\".format(dtime.month)+\"{:02d}\".format(dtime.day)+\"{:02d}\".format(i)+'00.csv', \n",
    "                              sep=',').drop(columns=[\"Unnamed: 6\"])\n",
    "        df_temp_CAPE_H0_DLS = pd.read_csv(fold + f'data/SPHERA/sphera_indices_grid{sp_res}km/CAPE_H0_DLS/{dtime.year}/ind_'+\n",
    "                              f'{dtime.year}'+\"{:02d}\".format(dtime.month)+\"{:02d}\".format(dtime.day)+\"{:02d}\".format(i)+'00_CAPE_H0_DLS.csv', \n",
    "                              sep=',').drop(columns=[\"Unnamed: 6\"])\n",
    "\n",
    "        df_temp = pd.concat([df_temp_SLI_K_AvvGeop_VV700, df_temp_CAPE_H0_DLS[['DeepShear','H0','CAPE_MU','CAPE_ML']]],\n",
    "                            axis=1, join=\"inner\")\n",
    "        df_sp = df_sp.append(df_temp, ignore_index=True)\n",
    "\n",
    "    df_sp = df_sp.rename(columns={'Data' : 'datetime', 'Macroarea' : 'box_id', 'DeepShear' : 'DLS'})\n",
    "\n",
    "    df_sp['datetime'] = pd.to_datetime(df_sp['datetime'], format='%Y%m%d%H')\n",
    "\n",
    "    #drop all rows where one (and so all) of the parameters has huge values >1e38\n",
    "    df_sp = df_sp.drop(df_sp.loc[df_sp.LI > 1e38].index)\n",
    "    \n",
    "    #FIX BOX_ID INDEX: make it start from 0 and not 1 otherwise all grid cells are shifted to the left of 1!\n",
    "    df_sp.box_id = df_sp.box_id - 1 \n",
    "    \n",
    "    return df_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4c2f4a",
   "metadata": {},
   "source": [
    "### Functions for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c200452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOLD VERSION TO PLOT SPHERA GRIDDED DATA USING SHAPEFILE READ WITH \"shapefile\" AND NOT WITH \"geopandas\"\\nfor id in col_boxes[:-1]:\\n    shape_ex = sf_sp.shape(id)\\n    x_lon = np.zeros((len(shape_ex.points),1))\\n    y_lat = np.zeros((len(shape_ex.points),1))\\n    for ip in range(len(shape_ex.points)):\\n        x_lon[ip] = shape_ex.points[ip][0]\\n        y_lat[ip] = shape_ex.points[ip][1]\\n\\n    c_sp_par = ax.fill(x_lon,y_lat, color = color_ton[int(df_sp_Hsel.loc[df_sp_Hsel.box_id == id].index.values)], \\n                     alpha=1);  #0.8\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Function for plotting shapefiles OLD VERSION WHEN USING PACKAGE SHAPEFILE INSTEAD OF GEOPANDAS (this is deprecated now)\n",
    "\"\"\"\n",
    "\n",
    "#inputs: shapefile, linewidth and color\n",
    "\n",
    "def HF_plotshape(sf,lw=0.5,col='k',ls='--'):\n",
    "    for shape in sf.shapeRecords():   \n",
    "        \n",
    "        sxm=[];\n",
    "        for i in range(len(shape.shape.parts)):\n",
    "            i_start = shape.shape.parts[i]\n",
    "            if i==len(shape.shape.parts)-1:\n",
    "                i_end = len(shape.shape.points)\n",
    "            else:\n",
    "                i_end = shape.shape.parts[i+1]\n",
    "            sx = [i[0] for i in shape.shape.points[i_start:i_end]]\n",
    "            sy = [i[1] for i in shape.shape.points[i_start:i_end]]\n",
    "            pl.plot(sx,sy,col, linewidth=lw, linestyle=ls)\n",
    "            if (i_end-i_start+1)>len(sxm):\n",
    "                im=i; sxm=sx;sym=sy;\n",
    "                \n",
    "\"\"\"\n",
    "OLD VERSION TO PLOT SPHERA GRIDDED DATA USING SHAPEFILE READ WITH \"shapefile\" AND NOT WITH \"geopandas\"\n",
    "for id in col_boxes[:-1]:\n",
    "    shape_ex = sf_sp.shape(id)\n",
    "    x_lon = np.zeros((len(shape_ex.points),1))\n",
    "    y_lat = np.zeros((len(shape_ex.points),1))\n",
    "    for ip in range(len(shape_ex.points)):\n",
    "        x_lon[ip] = shape_ex.points[ip][0]\n",
    "        y_lat[ip] = shape_ex.points[ip][1]\n",
    "\n",
    "    c_sp_par = ax.fill(x_lon,y_lat, color = color_ton[int(df_sp_Hsel.loc[df_sp_Hsel.box_id == id].index.values)], \n",
    "                     alpha=1);  #0.8\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54e7e0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function for color-coding gridded data (as Lampinet based on N of lightnings on a box, or LJ index) \n",
    "\"\"\"\n",
    "\n",
    "def HF_calc_color(data, color=None, proxy_intervals=None):\n",
    "    \n",
    "    #LAMPINET 'raw' daily data:\n",
    "    if color == 1:\n",
    "        nbins=5\n",
    "        color_sq =['#F1F1F1','#D2E7ED','#B7D5E4','#A0BDD8','#8DA3CA','#8085B8','#7665A4','#6F418D','#6B0077']\n",
    "    \n",
    "    #LJ index: boolean index (0 or 1 only)    \n",
    "    elif color == 2:\n",
    "        nbins=2\n",
    "        color_sq=['white','black']\n",
    "    \n",
    "    #SPHERA proxy: LI\n",
    "    elif color == 3:\n",
    "        proxy_intervals = [-np.inf,-7,-5,-3,0,2,np.inf] \n",
    "        color_sq=['#5B3794','#87489D','#AA5FA5','#C87AAD','#E198B5','#F4B8C0','#F8DCD9']\n",
    "\n",
    "        #define finite LI_intervals used for colorbar: remove final element np.inf and substitie first element \n",
    "        #-np.inf with a finite element (-10 for LI):\n",
    "        low_bound = -10\n",
    "        proxy_intervals_fin = proxy_intervals[:-1][1:]\n",
    "        proxy_intervals_fin.insert(0,low_bound)\n",
    "    \n",
    "    #SPHERA proxy: Kindex\n",
    "    elif color == 4:\n",
    "        #proxy_intervals = [-np.inf,10,20,27,32,38,45,np.inf] \n",
    "        proxy_intervals = [-np.inf,10,20,26,30,33,36,39,41,np.inf] \n",
    "        \n",
    "        #brown color coding\n",
    "        #color_sq = ['#F6CE94','#EBB47C','#D99463','#C06F49','#A0432D','#7A000E','#300106']\n",
    "        #color_sq = ['#F2F2E2','#F9EEC0','#FAE2AA','#F6CE94','#EBB47C','#D99463','#C06F49','#A0432D','#7A000E']\n",
    "       \n",
    "        #Purple-Yellow (advanced) colord coding (hclwizard)\n",
    "        color_sq = ['#F1F1F1','#E4F3D5','#B5F5C8','#5CEFD1','#00DEE8','#00BEFE','#5388FF','#9C3FCB','#8F007D']\n",
    "    \n",
    "        #define finite intervals used for colorbar:\n",
    "        proxy_intervals_fin = proxy_intervals[:-1][1:]\n",
    "        \n",
    "    #SPHERA proxy: %VV700    \n",
    "    elif color == 5:    \n",
    "        proxy_intervals = [-1,15,30,45,60,75,90,100]   #should start from 0 but starting from 0 some values == 0 are excluded\n",
    "    \n",
    "        #red colorcoding\n",
    "        color_sq = ['#F1F1F1','#DCC2C3','#C7989A','#B17072','#9A484C','#811C24','#690000']\n",
    "        \n",
    "        #define finite intervals used for colorbar:\n",
    "        low_bound = 0\n",
    "        proxy_intervals_fin = proxy_intervals[1:]\n",
    "        proxy_intervals_fin.insert(0,low_bound)\n",
    "      \n",
    "    #SPHERA proxy: CAPE\n",
    "    elif color == 6: \n",
    "        proxy_intervals = [-1,400,800,1200,1600,2000,2400,2800,3200,np.inf] \n",
    "        \n",
    "        #sunset colorcoding\n",
    "        color_sq = ['#FFFFB5','#FFE392','#FFC38D','#FFA698','#F68BA4','#DD75AB','#BF62AD','#9C55A8','#704D9E']\n",
    "        \n",
    "        #define finite intervals used for colorbar:\n",
    "        low_bound = 0\n",
    "        proxy_intervals_fin = proxy_intervals[:-1][1:]\n",
    "        proxy_intervals_fin.insert(0,low_bound)\n",
    "    \n",
    "    #SPHERA proxy: H0\n",
    "    elif color == 7:\n",
    "        proxy_intervals = [0,2500,2800,3100,3400,3700,4000,np.inf]\n",
    "    \n",
    "        #colorcoding\n",
    "        color_sq = ['#490062','#7F197E','#BC3699','#F55BA6','#FF98AA','#FFCBBF','#FCF5F2']\n",
    "        \n",
    "        #define finite intervals used for colorbar:\n",
    "        proxy_intervals_fin = proxy_intervals[:-1][1:]\n",
    "    \n",
    "    #SPHERA proxy: DLS\n",
    "    elif color == 8:\n",
    "        proxy_intervals = [-np.inf,4,8,12,16,20,24,28,np.inf]\n",
    "    \n",
    "        #colorcoding\n",
    "        color_sq = ['#F5F2D8','#C6E8BC','#7ED5B8','#34B8C0','#478EC1','#7A55AB','#80146E','#520945']\n",
    "        \n",
    "        #define finite intervals used for colorbar:\n",
    "        proxy_intervals_fin = proxy_intervals[:-1][1:]\n",
    "      \n",
    "    #OT max probability\n",
    "    elif color == 9:\n",
    "        proxy_intervals = [0.5,0.6,0.7,0.8,0.9,1.0]\n",
    "    \n",
    "        #colorcoding\n",
    "        color_sq = ['#CAFBB9','#99E489','#72B96F','#42824A','#004616']\n",
    "        \n",
    "        #define finite intervals used for colorbar:\n",
    "        proxy_intervals_fin = proxy_intervals   \n",
    "        \n",
    "        \n",
    "    if color == 1:      \n",
    "        #color-code data based on intervals defined by quantile discretization of the data distribution\n",
    "        new_data, bins = pd.qcut(data, nbins, retbins=True, labels=list(range(nbins)), duplicates='drop')\n",
    "        color_ton = []\n",
    "        for val in new_data:\n",
    "            color_ton.append(color_sq[val]) \n",
    "            \n",
    "    elif color == 2:\n",
    "        color_ton = []\n",
    "        for val in data:\n",
    "            color_ton.append(color_sq[val]) \n",
    "        bins = nbins\n",
    "    \n",
    "    elif (color == 3) or (color == 4) or (color == 5) or (color == 6) or (color == 7) or (color == 8) or (color == 9):\n",
    "        #color-code data based on pre-defined intervals incl -inf and +inf\n",
    "        new_data, bins = pd.cut(data, proxy_intervals, labels=list(range(len(proxy_intervals)-1)), retbins=True)\n",
    "        colors = color_sq\n",
    "        color_ton = []\n",
    "        for val in new_data:\n",
    "            color_ton.append(color_sq[val]) \n",
    "        bins = proxy_intervals_fin\n",
    "   \n",
    "    return color_ton, bins, color_sq;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29ccc55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function for plotting LAMPINET data: choice between daily sum of strikes or LJ index for a specific hour of the day\n",
    "\"\"\"\n",
    "\n",
    "def HF_LAMPINET_plot(sf_lamp, sf_geo, x_lim, y_lim, df_lamp_sel, u_ev_sel,eswd_ev_sel, eswd_trange, clonsel, clatsel, \n",
    "                     cnamesel, LJ, ESWD=True):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize = (12,8))  #12\n",
    "\n",
    "    for shape in sf_lamp.shapeRecords():\n",
    "        x = [i[0] for i in shape.shape.points[:]]\n",
    "        y = [i[1] for i in shape.shape.points[:]]\n",
    "\n",
    "    if (x_lim != None) & (y_lim != None):     \n",
    "        plt.xlim(x_lim)\n",
    "        plt.ylim(y_lim)\n",
    "\n",
    "    sf_geo[0].plot(ax=ax, alpha = 0.5, facecolor = 'none', lw = 0.5, zorder=2)\n",
    "    if len(sf_geo) > 1:\n",
    "        sf_geo[1].plot(ax=ax, alpha = 0.5, facecolor = 'none', lw = 0.5, zorder=2)\n",
    "\n",
    "    #fill with color boxes\n",
    "    col_boxes = df_lamp_sel.box_id\n",
    "    \n",
    "    #condition on: are treating hourly LJ index values or daily total number of lightning strikes?\n",
    "    if LJ == True:\n",
    "        color_ton, bins, colors = HF_calc_color(df_lamp_sel.ind, color=2)\n",
    "    else:\n",
    "        color_ton, bins, colors = HF_calc_color(df_lamp_sel.N_lightnings, color=1)  \n",
    "\n",
    "    for id in col_boxes[:-1]:\n",
    "        shape_ex = sf_lamp.shape(id)\n",
    "        x_lon = np.zeros((len(shape_ex.points),1))\n",
    "        y_lat = np.zeros((len(shape_ex.points),1))\n",
    "        for ip in range(len(shape_ex.points)):\n",
    "            x_lon[ip] = shape_ex.points[ip][0]\n",
    "            y_lat[ip] = shape_ex.points[ip][1]\n",
    "        c_lamp = ax.fill(x_lon,y_lat, color = color_ton[int(df_lamp_sel.loc[df_lamp_sel.box_id == id].index.values)], \n",
    "                         alpha=0.8);\n",
    "        \n",
    "    pl.grid(ls=(0,(5,5)))\n",
    "    \n",
    "    if LJ==False:\n",
    "        #add colorbar\n",
    "        cmap_lamp = matplotlib.colors.ListedColormap(sns.color_palette(colors).as_hex())    \n",
    "\n",
    "        img = plt.imshow(np.array([[0,1,2,3,4,5,6,7,8,9]]), cmap=cmap_lamp) #img: \"fake\" image to plot only to consider cbar\n",
    "        img.set_visible(False)\n",
    "\n",
    "        cb_lamp = plt.colorbar(orientation=\"vertical\", shrink=0.7)\n",
    "        cb_lamp.ax.set_yticklabels(bins.astype(int), fontsize=13)\n",
    "        cb_lamp.ax.set_ylabel('Lightning strikes per day', fontsize=15);\n",
    "    else:\n",
    "        plt.title(f'{df_lamp_sel.datetime[0]}')\n",
    "        \n",
    "    '''\n",
    "    PLOT insurance/ESWD data (punctual)\n",
    "    '''\n",
    "    if ESWD == True:\n",
    "        \n",
    "        #Plot insurance/eswd data with colorbar varying on hour of the day\n",
    "        \n",
    "        #find beginning and end of datetime of insurance claims\n",
    "        if len(u_ev_sel)>0:\n",
    "            dt_dataset = u_ev_sel\n",
    "        else:\n",
    "            dt_dataset = eswd_ev_sel\n",
    "        \n",
    "        #divide eswd dataset in datasets with info on size of hail or not:\n",
    "        if len(eswd_ev_sel)>0:    \n",
    "            eswd_ev_sel_NOsize = eswd_ev_sel.loc[np.isnan(eswd_ev_sel['size']) == True].reset_index(drop=True)\n",
    "            eswd_ev_sel_wsize = eswd_ev_sel.loc[np.isnan(eswd_ev_sel['size']) == False].reset_index(drop=True)\n",
    "        \n",
    "        cmap = plt.cm.autumn\n",
    "        \n",
    "        #Conditions: if all eswd/ins data are relative to the same datetime (including also the case of 1 only data),\n",
    "        #or if all data have datetimes differing only of less than eswd_trange (e.g. 30 minutes) -> colorcode with\n",
    "        #only one color, otherwise create colorbar:\n",
    "        \n",
    "        if (all(i == dt_dataset.datetime[0] for i in dt_dataset.datetime) == False):  \n",
    "        \n",
    "            beg = dt_dataset.datetime.loc[dt_dataset.index[0]]\n",
    "            end = dt_dataset.datetime.iloc[-1]\n",
    "\n",
    "            tdelta = pd.Series(end-beg).dt.round(eswd_trange)  #how many hours of reports, depending on eswd_trange input\n",
    "\n",
    "            if eswd_trange == '60min':   #'H'\n",
    "                eswd_tindex = 1\n",
    "            elif eswd_trange == '30min':\n",
    "                eswd_tindex = 2\n",
    "            \n",
    "            if pd.to_timedelta(tdelta.values) > pd.to_timedelta(eswd_trange):\n",
    "            \n",
    "                cbar_plot = True\n",
    "                #list of dates to be used in colorbar: one every eswd_trange starting from the first value and \n",
    "                #arriving to end:\n",
    "                date_list = pd.Series([beg])\n",
    "                for i in np.arange(1,int(tdelta.astype('timedelta64[h]')+1)*eswd_tindex):\n",
    "                    date_list = date_list.append(pd.Series([beg + timedelta(minutes=int(i)*60/eswd_tindex)]))\n",
    "                date_list = date_list.reset_index(drop=True)\n",
    "\n",
    "                #set extreme values of colorbar\n",
    "                num_date_list = pd.to_numeric(date_list)\n",
    "                vmin=pd.to_numeric(num_date_list[0])\n",
    "                vmax=pd.to_numeric(num_date_list[len(date_list) - 1])\n",
    "                tdelta_cb = int(pd.to_numeric(pd.Series(timedelta(minutes=int(60/eswd_tindex)))))\n",
    "\n",
    "                norm = matplotlib.colors.BoundaryNorm(np.arange(vmin, vmax+tdelta_cb, tdelta_cb), cmap.N)\n",
    "                \n",
    "                if len(u_ev_sel)>0:\n",
    "                    col_code_u = u_ev_sel['datetime']\n",
    "                \n",
    "                if len(eswd_ev_sel)>0:    \n",
    "                    col_code_e_NOsize = eswd_ev_sel_NOsize['datetime']\n",
    "                    col_code_e_wsize = eswd_ev_sel_wsize['datetime']\n",
    "                    \n",
    "            else:\n",
    "                cbar_plot = False\n",
    "                norm = None\n",
    "                \n",
    "                if len(u_ev_sel)>0:\n",
    "                    col_code_u = 'red'\n",
    "                \n",
    "                if len(eswd_ev_sel)>0:    \n",
    "                    col_code_e_NOsize = 'red'\n",
    "                    col_code_e_wsize = 'red'\n",
    "                \n",
    "        else:\n",
    "            cbar_plot = False\n",
    "            norm = None\n",
    "                \n",
    "            if len(u_ev_sel)>0:\n",
    "                col_code_u = 'red'\n",
    "\n",
    "            if len(eswd_ev_sel)>0:    \n",
    "                col_code_e_NOsize = 'red'\n",
    "                col_code_e_wsize = 'red'  \n",
    "        \n",
    "        #plot locations of UNIPOL reports\n",
    "        if len(u_ev_sel)>0:\n",
    "            c4=pl.scatter(u_ev_sel.lon, u_ev_sel.lat,60,c=col_code_u,cmap=cmap,marker=\"o\",edgecolor='k', \n",
    "                          alpha=0.8, label='Insurance claims',norm=norm, zorder=4)    \n",
    "\n",
    "        #plot locations of ESWD hail reports with symbol proportional to the size of reported hail (or with different color)\n",
    "        if len(eswd_ev_sel)>0:\n",
    "            \n",
    "            eswd_ev_sel_NOsize = eswd_ev_sel.loc[np.isnan(eswd_ev_sel['size']) == True].reset_index(drop=True)\n",
    "            eswd_ev_sel_wsize = eswd_ev_sel.loc[np.isnan(eswd_ev_sel['size']) == False].reset_index(drop=True)\n",
    "            \n",
    "            if len(eswd_ev_sel_NOsize) > 0:\n",
    "                    c3_1=pl.scatter(eswd_ev_sel_NOsize.lon,eswd_ev_sel_NOsize.lat,35,c=col_code_e_NOsize,\n",
    "                            cmap=cmap,norm=norm,marker=\"v\",edgecolor='k',alpha=0.9,zorder=4)\n",
    "            if len(eswd_ev_sel_wsize) > 0:\n",
    "                    c3_2=pl.scatter(eswd_ev_sel_wsize.lon,eswd_ev_sel_wsize.lat,25*eswd_ev_sel_wsize['size'],\n",
    "                                    c=col_code_e_wsize,cmap=cmap,norm=norm,marker=\"^\",\n",
    "                                    edgecolor='k',alpha=0.9,zorder=4)\n",
    "        \n",
    "        #add colorbar (depending on eswd or unipol datasets)\n",
    "        if cbar_plot == True:\n",
    "            if len(eswd_ev_sel) > len(u_ev_sel):\n",
    "                c_bar = c3_2\n",
    "            else:\n",
    "                c_bar = c4\n",
    "\n",
    "            cb = pl.colorbar(c_bar, orientation='vertical',norm=norm, pad=0.025)\n",
    "            cb.ax.get_yaxis().set_ticks(pd.to_numeric(date_list))\n",
    "            cb.ax.set_yticklabels(date_list.dt.strftime('%H.%M')[:], fontsize=13)\n",
    "            cb.ax.set_ylabel('Hail report timing (UTC)', fontsize=15)\n",
    "\n",
    "        unip = mlines.Line2D([], [], color='orange', marker='o', markersize=8, markeredgecolor='k', ls='', \n",
    "                             label='Insurance claims')\n",
    "        eswd_1 = mlines.Line2D([], [], color='orange', marker='^', markersize=11, markeredgecolor='k', ls='', \n",
    "                     label='ESWD reports \\n prop. to hail size')\n",
    "        eswd_2 = mlines.Line2D([], [], color='orange', marker='v', markersize=11, markeredgecolor='k', ls='', \n",
    "                             label='ESWD reports \\n no info on hail size')\n",
    "        pl.legend(handles=[unip, eswd_1, eswd_2],loc='best', fontsize=12);\n",
    "        \n",
    "    #Plot locations and labels of selected cities\n",
    "    for k in range(len(clonsel)):  \n",
    "        pl.plot(clonsel[k],clatsel[k],'ko',markersize=6)\n",
    "        pl.plot(clonsel[k],clatsel[k],'wo',markersize=5)\n",
    "        pl.text(clonsel[k]+.055,clatsel[k]-.055, cnamesel[k],color='w',size=14)\n",
    "        pl.text(clonsel[k]+.05,clatsel[k]-.05, cnamesel[k],size=14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9e010bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function for plotting OT data (OT occurrence max probability) + EUCLID + ESWD/UNIPOL + LAMPINET\n",
    "\"\"\"\n",
    "\n",
    "def HF_OTmax_plot_wLightning(day, sf_lamp, sf_geo, x_lim, y_lim, df_lamp_sel, cpclon, cpclat, otpmax, df_lg_sel, df_lg_trange,\n",
    "                  u_ev_sel,eswd_ev_sel, eswd_trange, clonsel, clatsel, cnamesel, LJ=False, LAMPINET=True, OT=True, \n",
    "                  EUCLID=True, ESWD=True):\n",
    "    \n",
    "    ds=str(day)\n",
    "    \n",
    "    #Choose starting day to make selection (1 day ahead of this date)\n",
    "    dp=datetime.datetime.strptime(ds,'%Y%j')\n",
    "    dstr=dp.isoformat()[0:10] #datetime converted in string\n",
    "\n",
    "    #parameter to shrink all colorbars to same size\n",
    "    shrink_cb=0.7\n",
    "    pad_cb=-0.005\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (13,8))\n",
    "\n",
    "    if (x_lim != None) & (y_lim != None):     \n",
    "        plt.xlim(x_lim)\n",
    "        plt.ylim(y_lim)\n",
    "        \n",
    "    '''\n",
    "    PLOT shapefile\n",
    "    '''\n",
    "    sf_geo[0].plot(ax=ax, alpha = 0.5, facecolor = 'none', lw = 0.5, zorder=2)\n",
    "    if len(sf_geo) > 1:\n",
    "        sf_geo[1].plot(ax=ax, alpha = 0.5, facecolor = 'none', lw = 0.5, zorder=2)\n",
    "   \n",
    "    '''\n",
    "    PLOT Lampinet LJ index (gridded)\n",
    "    '''\n",
    "    \n",
    "    if LAMPINET == True:\n",
    "        \n",
    "        #fill with color boxes\n",
    "        col_boxes = df_lamp_sel.box_id\n",
    "\n",
    "        if LJ == True:\n",
    "            color_ton, bins, colors = HF_calc_color(df_lamp_sel.ind, color=2)\n",
    "        else:\n",
    "            color_ton, bins, colors = HF_calc_color(df_lamp_sel.N_lightnings, color=1)  \n",
    "\n",
    "        for id in col_boxes[:-1]:  \n",
    "            shape_ex = sf_lamp.shape(id)\n",
    "            x_lon = np.zeros((len(shape_ex.points),1))\n",
    "            y_lat = np.zeros((len(shape_ex.points),1))\n",
    "            for ip in range(len(shape_ex.points)):\n",
    "                x_lon[ip] = shape_ex.points[ip][0]\n",
    "                y_lat[ip] = shape_ex.points[ip][1]\n",
    "            c_lamp = ax.fill(x_lon,y_lat, color = color_ton[int(df_lamp_sel.loc[df_lamp_sel.box_id == id].index.values)], \n",
    "                             alpha=0.8, zorder=1);\n",
    "\n",
    "        if LJ==False:\n",
    "            #add colorbar\n",
    "            cmap_lamp = matplotlib.colors.ListedColormap(sns.color_palette(colors).as_hex())    \n",
    "\n",
    "            img = plt.imshow(np.array([[0,1,2,3,4,5,6,7,8,9]]), cmap=cmap_lamp) #img: \"fake\" image to plot only to consider cbar\n",
    "            img.set_visible(False)\n",
    "\n",
    "            cb_lamp = plt.colorbar(orientation=\"vertical\", shrink=shrink_cb, pad=pad_cb)\n",
    "            cb_lamp.ax.set_yticklabels(bins.astype(int), fontsize=13)\n",
    "            cb_lamp.ax.set_ylabel('Lightning strikes per day (LAMPINET - 10km grid)', fontsize=15);\n",
    "\n",
    "    '''\n",
    "    PLOT OT probability (gridded) with colorbar\n",
    "    '''\n",
    "    if OT == True:\n",
    "        \n",
    "        \"\"\" THIS SHOULD BE CHANGED USING XARRAY AND PCOLORMESH!!!\"\"\"\n",
    "        \n",
    "        cm=pl.pcolor(cpclon,cpclat,otpmax,cmap=\"Greens\",vmin=0,vmax=100, zorder=2, alpha=0.9)\n",
    "        \n",
    "        \"\"\" THIS SHOULD BE CHANGED USING XARRAY AND PCOLORMESH!!!\"\"\"\n",
    "        \n",
    "        pl.clim([0.5,1])\n",
    "        cb_ot = pl.colorbar(shrink=shrink_cb, pad=pad_cb)\n",
    "        cb_ot.ax.tick_params(axis='both', which='major', labelsize=13)\n",
    "        cb_ot.ax.set_ylabel('OT max probability', fontsize=15);\n",
    "\n",
    "\n",
    "    '''\n",
    "    PLOT EUCLID lightning data (punctual)\n",
    "    '''\n",
    "    #Plot lightning data with colorbar varying on hour of the day\n",
    "    #find beginning and end of datetime of insurance claims\n",
    "    if EUCLID == True:\n",
    "\n",
    "        if len(df_lg_sel)>0:\n",
    "\n",
    "            beg_l = df_lg_sel.datetime.loc[df_lg_sel.index[0]]\n",
    "            end_l = df_lg_sel.datetime.iloc[-1]\n",
    "            \n",
    "            tdelta_l = pd.Series(end_l-beg_l).dt.round(df_lg_trange)  #how many hours of reports, depending on df_lg_trange input\n",
    "        \n",
    "            if df_lg_trange == '60min':   #'H'\n",
    "                df_lg_tindex = 1\n",
    "            elif df_lg_trange == '30min':\n",
    "                df_lg_tindex = 2\n",
    "\n",
    "            #list of dates to be used in colorbar: one every hour starting from the first value and arriving to end:\n",
    "            date_list_l = pd.Series([beg_l])\n",
    "            for i in np.arange(1,int(tdelta_l.astype('timedelta64[h]')+1)*df_lg_tindex):\n",
    "                date_list_l = date_list_l.append(pd.Series([beg_l + timedelta(minutes=int(i)*60/df_lg_tindex)]))\n",
    "            date_list_l = date_list_l.reset_index(drop=True)\n",
    "\n",
    "            #set extreme values of colorbar\n",
    "            num_date_list_l = pd.to_numeric(date_list_l)\n",
    "            vmin_l=pd.to_numeric(num_date_list_l[0])\n",
    "            vmax_l=pd.to_numeric(num_date_list_l[len(date_list_l) - 1])\n",
    "            tdelta_cb_l = int(pd.to_numeric(pd.Series(timedelta(minutes=int(60/df_lg_tindex)))))\n",
    "            \n",
    "            cmap_l = plt.cm.spring\n",
    "            norm_l =matplotlib.colors.BoundaryNorm(np.arange(vmin_l, vmax_l+tdelta_cb_l, tdelta_cb_l), cmap_l.N)\n",
    "\n",
    "            #plot locations of lightnings\n",
    "            c5=pl.scatter(df_lg_sel.lon, df_lg_sel.lat,10,c=df_lg_sel['datetime'],cmap=cmap_l,marker=\".\",\n",
    "                          alpha=0.8, label='Lightnings',norm=norm_l, zorder=3)  \n",
    "\n",
    "            #add colorbar\n",
    "            cb_l = pl.colorbar(c5, orientation='vertical',norm=norm_l, shrink=shrink_cb, pad=pad_cb)\n",
    "            cb_l.ax.get_yaxis().set_ticks(pd.to_numeric(date_list_l))\n",
    "            cb_l.ax.set_yticklabels(date_list_l.dt.strftime('%H.%M')[:], fontsize=13)\n",
    "            cb_l.ax.set_ylabel('EUCLID lightning strikes timing (UTC)', fontsize=15)\n",
    "\n",
    "    '''\n",
    "    PLOT insurance/ESWD data (punctual)\n",
    "    '''\n",
    "    if ESWD == True:\n",
    "        \n",
    "        #Plot insurance/eswd data with colorbar varying on hour of the day\n",
    "        \n",
    "        #find beginning and end of datetime of insurance claims\n",
    "        if len(u_ev_sel)>0:\n",
    "            dt_dataset = u_ev_sel\n",
    "        else:\n",
    "            dt_dataset = eswd_ev_sel\n",
    "        \n",
    "        #divide eswd dataset in datasets with info on size of hail or not:\n",
    "        if len(eswd_ev_sel)>0:    \n",
    "            eswd_ev_sel_NOsize = eswd_ev_sel.loc[np.isnan(eswd_ev_sel['size']) == True].reset_index(drop=True)\n",
    "            eswd_ev_sel_wsize = eswd_ev_sel.loc[np.isnan(eswd_ev_sel['size']) == False].reset_index(drop=True)\n",
    "        \n",
    "        cmap = plt.cm.autumn\n",
    "        \n",
    "        #Conditions: if all eswd/ins data are relative to the same datetime (including also the case of 1 only data),\n",
    "        #or if all data have datetimes differing only of less than eswd_trange (e.g. 30 minutes) -> colorcode with\n",
    "        #only one color, otherwise create colorbar:\n",
    "        \n",
    "        if (all(i == dt_dataset.datetime[0] for i in dt_dataset.datetime) == False):  \n",
    "        \n",
    "            beg = dt_dataset.datetime.loc[dt_dataset.index[0]]\n",
    "            end = dt_dataset.datetime.iloc[-1]\n",
    "\n",
    "            tdelta = pd.Series(end-beg).dt.round(eswd_trange)  #how many hours of reports, depending on eswd_trange input\n",
    "\n",
    "            if eswd_trange == '60min':   #'H'\n",
    "                eswd_tindex = 1\n",
    "            elif eswd_trange == '30min':\n",
    "                eswd_tindex = 2\n",
    "            \n",
    "            if pd.to_timedelta(tdelta.values) > pd.to_timedelta(eswd_trange):\n",
    "            \n",
    "                cbar_plot = True\n",
    "                #list of dates to be used in colorbar: one every eswd_trange starting from the first value and \n",
    "                #arriving to end:\n",
    "                date_list = pd.Series([beg])\n",
    "                for i in np.arange(1,int(tdelta.astype('timedelta64[h]')+1)*eswd_tindex):\n",
    "                    date_list = date_list.append(pd.Series([beg + timedelta(minutes=int(i)*60/eswd_tindex)]))\n",
    "                date_list = date_list.reset_index(drop=True)\n",
    "\n",
    "                #set extreme values of colorbar\n",
    "                num_date_list = pd.to_numeric(date_list)\n",
    "                vmin=pd.to_numeric(num_date_list[0])\n",
    "                vmax=pd.to_numeric(num_date_list[len(date_list) - 1])\n",
    "                tdelta_cb = int(pd.to_numeric(pd.Series(timedelta(minutes=int(60/eswd_tindex)))))\n",
    "\n",
    "                norm = matplotlib.colors.BoundaryNorm(np.arange(vmin, vmax+tdelta_cb, tdelta_cb), cmap.N)\n",
    "                \n",
    "                if len(u_ev_sel)>0:\n",
    "                    col_code_u = u_ev_sel['datetime']\n",
    "                \n",
    "                if len(eswd_ev_sel)>0:    \n",
    "                    col_code_e_NOsize = eswd_ev_sel_NOsize['datetime']\n",
    "                    col_code_e_wsize = eswd_ev_sel_wsize['datetime']\n",
    "                    \n",
    "            else:\n",
    "                cbar_plot = False\n",
    "                norm = None\n",
    "                \n",
    "                if len(u_ev_sel)>0:\n",
    "                    col_code_u = 'red'\n",
    "                \n",
    "                if len(eswd_ev_sel)>0:    \n",
    "                    col_code_e_NOsize = 'red'\n",
    "                    col_code_e_wsize = 'red'\n",
    "                \n",
    "        else:\n",
    "            cbar_plot = False\n",
    "            norm = None\n",
    "                \n",
    "            if len(u_ev_sel)>0:\n",
    "                col_code_u = 'red'\n",
    "\n",
    "            if len(eswd_ev_sel)>0:    \n",
    "                col_code_e_NOsize = 'red'\n",
    "                col_code_e_wsize = 'red'  \n",
    "        \n",
    "        #plot locations of UNIPOL reports\n",
    "        if len(u_ev_sel)>0:\n",
    "            c4=pl.scatter(u_ev_sel.lon, u_ev_sel.lat,60,c=col_code_u,cmap=cmap,marker=\"o\",edgecolor='k', \n",
    "                          alpha=0.8, label='Insurance claims',norm=norm, zorder=4)    \n",
    "\n",
    "        #plot locations of ESWD hail reports with symbol proportional to the size of reported hail (or with different color)\n",
    "        if len(eswd_ev_sel)>0:\n",
    "            \n",
    "            eswd_ev_sel_NOsize = eswd_ev_sel.loc[np.isnan(eswd_ev_sel['size']) == True].reset_index(drop=True)\n",
    "            eswd_ev_sel_wsize = eswd_ev_sel.loc[np.isnan(eswd_ev_sel['size']) == False].reset_index(drop=True)\n",
    "            \n",
    "            if len(eswd_ev_sel_NOsize) > 0:\n",
    "                    c3_1=pl.scatter(eswd_ev_sel_NOsize.lon,eswd_ev_sel_NOsize.lat,35,c=col_code_e_NOsize,\n",
    "                            cmap=cmap,norm=norm,marker=\"v\",edgecolor='k',alpha=0.9,zorder=4)\n",
    "            if len(eswd_ev_sel_wsize) > 0:\n",
    "                    c3_2=pl.scatter(eswd_ev_sel_wsize.lon,eswd_ev_sel_wsize.lat,25*eswd_ev_sel_wsize['size'],\n",
    "                                    c=col_code_e_wsize,cmap=cmap,norm=norm,marker=\"^\",\n",
    "                                    edgecolor='k',alpha=0.9,zorder=4)\n",
    "        \n",
    "        #add colorbar (depending on eswd or unipol datasets)\n",
    "        if cbar_plot == True:\n",
    "            if len(eswd_ev_sel) > len(u_ev_sel):\n",
    "                c_bar = c3_2\n",
    "            else:\n",
    "                c_bar = c4\n",
    "\n",
    "            cb = pl.colorbar(c_bar, orientation='vertical',norm=norm, shrink=shrink_cb, pad=0.025)\n",
    "            cb.ax.get_yaxis().set_ticks(pd.to_numeric(date_list))\n",
    "            cb.ax.set_yticklabels(date_list.dt.strftime('%H.%M')[:], fontsize=13)\n",
    "            cb.ax.set_ylabel('Hail report timing (UTC)', fontsize=15)\n",
    "\n",
    "        unip = mlines.Line2D([], [], color='orange', marker='o', markersize=8, markeredgecolor='k', ls='', \n",
    "                             label='Insurance claims')\n",
    "        eswd_1 = mlines.Line2D([], [], color='orange', marker='^', markersize=11, markeredgecolor='k', ls='', \n",
    "                     label='ESWD reports \\n prop. to hail size')\n",
    "        eswd_2 = mlines.Line2D([], [], color='orange', marker='v', markersize=11, markeredgecolor='k', ls='', \n",
    "                             label='ESWD reports \\n no info on hail size')\n",
    "        pl.legend(handles=[unip, eswd_1, eswd_2],loc='upper right', fontsize=12);\n",
    "\n",
    "    #Plot locations and labels of selected cities\n",
    "    for k in range(len(clonsel)):  \n",
    "        pl.plot(clonsel[k],clatsel[k],'ko',markersize=6)\n",
    "        pl.plot(clonsel[k],clatsel[k],'wo',markersize=5)\n",
    "        pl.text(clonsel[k]+.055,clatsel[k]-.055, cnamesel[k],color='w',size=14)\n",
    "        pl.text(clonsel[k]+.05,clatsel[k]-.05, cnamesel[k],size=14)\n",
    "    \n",
    "    pl.grid(ls=(0,(5,5)))\n",
    "    pl.title('Event '+str(dstr), fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c13f415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function as before for plotting OT data (OT occurrence max probability) + ESWD/UNIPOL without lightning data\n",
    "\"\"\"\n",
    "\n",
    "def HF_OTmax_plot(day, sf_geo, x_lim, y_lim, cpclon, cpclat, otpmax, u_ev_sel, eswd_ev_sel, eswd_trange, hhmin, hhmax,\n",
    "                  clonsel, clatsel, cnamesel, OT=True, ESWD=True):\n",
    "  \n",
    "    ds=str(day)\n",
    "    \n",
    "    #Choose starting day to make selection (1 day ahead of this date)\n",
    "    dp=datetime.datetime.strptime(ds,'%Y%j')\n",
    "    dstr=dp.isoformat()[0:10] #datetime converted in string\n",
    "\n",
    "    #parameter to shrink all colorbars to same size\n",
    "    #shrink_cb=0.7\n",
    "    #pad_cb=-0.005\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (12,8))\n",
    "\n",
    "    if (x_lim != None) & (y_lim != None):     \n",
    "        plt.xlim(x_lim)\n",
    "        plt.ylim(y_lim)\n",
    "\n",
    "    '''\n",
    "    PLOT shapefile\n",
    "    '''\n",
    "    sf_geo[0].plot(ax=ax, alpha = 0.5, facecolor = 'none', lw = 0.5, zorder=2)\n",
    "    if len(sf_geo) > 1:\n",
    "        sf_geo[1].plot(ax=ax, alpha = 0.5, facecolor = 'none', lw = 0.5, zorder=2)\n",
    "   \n",
    "    '''\n",
    "    PLOT OT probability (gridded) with colorbar\n",
    "    '''\n",
    "    if OT == True:\n",
    "        \"\"\" THIS SHOULD BE CHANGED USING XARRAY AND PCOLORMESH!!!\"\"\"\n",
    "        \n",
    "        cm=pl.pcolor(cpclon,cpclat,otpmax,cmap=\"Greens\",vmin=0,vmax=100, zorder=2, alpha=0.9)\n",
    "        \n",
    "        \"\"\" THIS SHOULD BE CHANGED USING XARRAY AND PCOLORMESH!!!\"\"\"\n",
    "\n",
    "        pl.clim([0.5,1])\n",
    "        cb_ot_ax = fig.add_axes([0.73, .12, .025, .76])\n",
    "        cb_ot = pl.colorbar(cax=cb_ot_ax)\n",
    "        cb_ot.ax.tick_params(axis='both', which='major', labelsize=13)\n",
    "        cb_ot.ax.set_ylabel('OT max probability', fontsize=15);\n",
    "\n",
    "    #return to figure axis from cax (colorbar axis)\n",
    "    plt.sca(ax)\n",
    "\n",
    "    '''\n",
    "    PLOT insurance/ESWD data (punctual)\n",
    "    '''\n",
    "    if ESWD == True:\n",
    "\n",
    "        #Plot insurance/eswd data with colorbar varying on hour of the day\n",
    "\n",
    "        #find beginning and end of datetime of insurance claims\n",
    "        if len(u_ev_sel)>0:\n",
    "            dt_dataset = u_ev_sel\n",
    "        else:\n",
    "            dt_dataset = eswd_ev_sel\n",
    "\n",
    "        #divide eswd dataset in datasets with info on size of hail or not:\n",
    "        if len(eswd_ev_sel)>0:    \n",
    "            eswd_ev_sel_NOsize = eswd_ev_sel.loc[np.isnan(eswd_ev_sel['size']) == True].reset_index(drop=True)\n",
    "            eswd_ev_sel_wsize = eswd_ev_sel.loc[np.isnan(eswd_ev_sel['size']) == False].reset_index(drop=True)\n",
    "\n",
    "        cmap = plt.cm.autumn\n",
    "\n",
    "        #Conditions: if all eswd/ins data are relative to the same datetime (including also the case of 1 only data),\n",
    "        #or if all data have datetimes differing only of less than eswd_trange (e.g. 30 minutes) -> colorcode with\n",
    "        #only one color, otherwise create colorbar:\n",
    "\n",
    "        if (all(i == dt_dataset.datetime[0] for i in dt_dataset.datetime) == False):  \n",
    "\n",
    "            beg = dt_dataset.datetime.loc[dt_dataset.index[0]]\n",
    "            end = dt_dataset.datetime.iloc[-1]\n",
    "\n",
    "            tdelta = pd.Series(end-beg).dt.round(eswd_trange)  #how many hours of reports, depending on eswd_trange input\n",
    "\n",
    "            if eswd_trange == '60min':   #'H'\n",
    "                eswd_tindex = 1\n",
    "            elif eswd_trange == '30min':\n",
    "                eswd_tindex = 2\n",
    "\n",
    "            if pd.to_timedelta(tdelta.values) > pd.to_timedelta(eswd_trange):\n",
    "\n",
    "                cbar_plot = True\n",
    "                #list of dates to be used in colorbar: one every eswd_trange starting from the first value and \n",
    "                #arriving to end:\n",
    "                date_list = pd.Series([beg])\n",
    "                for i in np.arange(1,int(tdelta.astype('timedelta64[h]')+1)*eswd_tindex):\n",
    "                    date_list = date_list.append(pd.Series([beg + timedelta(minutes=int(i)*60/eswd_tindex)]))\n",
    "                date_list = date_list.reset_index(drop=True)\n",
    "\n",
    "                #set extreme values of colorbar\n",
    "                num_date_list = pd.to_numeric(date_list)\n",
    "                vmin=pd.to_numeric(num_date_list[0])\n",
    "                vmax=pd.to_numeric(num_date_list[len(date_list) - 1])\n",
    "                tdelta_cb = int(pd.to_numeric(pd.Series(timedelta(minutes=int(60/eswd_tindex)))))\n",
    "\n",
    "                norm = matplotlib.colors.BoundaryNorm(np.arange(vmin, vmax+tdelta_cb, tdelta_cb), cmap.N)\n",
    "\n",
    "                if len(u_ev_sel)>0:\n",
    "                    col_code_u = u_ev_sel['datetime']\n",
    "\n",
    "                if len(eswd_ev_sel)>0:    \n",
    "                    col_code_e_NOsize = eswd_ev_sel_NOsize['datetime']\n",
    "                    col_code_e_wsize = eswd_ev_sel_wsize['datetime']\n",
    "\n",
    "            else:\n",
    "                cbar_plot = False\n",
    "                norm = None\n",
    "\n",
    "                if len(u_ev_sel)>0:\n",
    "                    col_code_u = 'red'\n",
    "\n",
    "                if len(eswd_ev_sel)>0:    \n",
    "                    col_code_e_NOsize = 'red'\n",
    "                    col_code_e_wsize = 'red'\n",
    "\n",
    "        else:\n",
    "            cbar_plot = False\n",
    "            norm = None\n",
    "\n",
    "            if len(u_ev_sel)>0:\n",
    "                col_code_u = 'red'\n",
    "\n",
    "            if len(eswd_ev_sel)>0:    \n",
    "                col_code_e_NOsize = 'red'\n",
    "                col_code_e_wsize = 'red'  \n",
    "\n",
    "        #plot locations of UNIPOL reports\n",
    "        if len(u_ev_sel)>0:\n",
    "            c4=pl.scatter(u_ev_sel.lon, u_ev_sel.lat,60,c=col_code_u,cmap=cmap,marker=\"o\",edgecolor='k', \n",
    "                          alpha=0.8, label='Insurance claims',norm=norm, zorder=4)    \n",
    "\n",
    "        #plot locations of ESWD hail reports with symbol proportional to the size of reported hail (or with different color)\n",
    "        if len(eswd_ev_sel)>0:\n",
    "\n",
    "            eswd_ev_sel_NOsize = eswd_ev_sel.loc[np.isnan(eswd_ev_sel['size']) == True].reset_index(drop=True)\n",
    "            eswd_ev_sel_wsize = eswd_ev_sel.loc[np.isnan(eswd_ev_sel['size']) == False].reset_index(drop=True)\n",
    "\n",
    "            if len(eswd_ev_sel_NOsize) > 0:\n",
    "                    c3_1=pl.scatter(eswd_ev_sel_NOsize.lon,eswd_ev_sel_NOsize.lat,35,c=col_code_e_NOsize,\n",
    "                            cmap=cmap,norm=norm,marker=\"v\",edgecolor='k',alpha=0.9,zorder=4)\n",
    "            if len(eswd_ev_sel_wsize) > 0:\n",
    "                    c3_2=pl.scatter(eswd_ev_sel_wsize.lon,eswd_ev_sel_wsize.lat,25*eswd_ev_sel_wsize['size'],\n",
    "                                    c=col_code_e_wsize,cmap=cmap,norm=norm,marker=\"^\",\n",
    "                                    edgecolor='k',alpha=0.9,zorder=4)\n",
    "\n",
    "        #add colorbar (depending on eswd or unipol datasets)\n",
    "        if cbar_plot == True:\n",
    "            if len(eswd_ev_sel) > len(u_ev_sel):\n",
    "                c_bar = c3_2\n",
    "            else:\n",
    "                c_bar = c4\n",
    "\n",
    "            cb_eswd_ax = fig.add_axes([0.82, .12, .025, .76])\n",
    "            cb = pl.colorbar(c_bar, orientation='vertical',norm=norm, cax=cb_eswd_ax)\n",
    "            cb.ax.get_yaxis().set_ticks(pd.to_numeric(date_list))\n",
    "            cb.ax.set_yticklabels(date_list.dt.strftime('%H.%M')[:], fontsize=13)\n",
    "            cb.ax.set_ylabel('Hail report timing (UTC)', fontsize=15)\n",
    "\n",
    "        #return to figure axis from cax (colorbar axis)\n",
    "        plt.sca(ax)\n",
    "\n",
    "        unip = mlines.Line2D([], [], color='orange', marker='o', markersize=8, markeredgecolor='k', ls='', \n",
    "                             label='Insurance claims')\n",
    "        eswd_1 = mlines.Line2D([], [], color='orange', marker='^', markersize=11, markeredgecolor='k', ls='', \n",
    "                     label='ESWD reports \\n prop. to hail size')\n",
    "        eswd_2 = mlines.Line2D([], [], color='orange', marker='v', markersize=11, markeredgecolor='k', ls='', \n",
    "                             label='ESWD reports \\n no info on hail size')\n",
    "        pl.legend(handles=[unip, eswd_1, eswd_2],loc='best', fontsize=12);\n",
    "\n",
    "\n",
    "    #Plot locations and labels of selected cities\n",
    "    for k in range(len(clonsel)):  \n",
    "        pl.plot(clonsel[k],clatsel[k],'ko',markersize=6)\n",
    "        pl.plot(clonsel[k],clatsel[k],'wo',markersize=5)\n",
    "        pl.text(clonsel[k]+.055,clatsel[k]-.055, cnamesel[k],color='w',size=14)\n",
    "        pl.text(clonsel[k]+.05,clatsel[k]-.05, cnamesel[k],size=14)\n",
    "\n",
    "    pl.grid(ls=(0,(5,5)))\n",
    "    pl.title('Event '+str(dstr) + ' ' + str(hhmin) + '-' + str(hhmax) + ' UTC', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b5233c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function for plotting OT data (DTmin) + ESWD/UNIPOL\n",
    "\"\"\"\n",
    "\n",
    "def HF_DTmin_plot(day, sf_geo, x_lim, y_lim, cpclon, cpclat, dtmin, df_lg_sel, df_lg_trange, u_ev_sel,\n",
    "                  eswd_ev_sel, eswd_trange, clonsel, clatsel, cnamesel, OT=True, EUCLID=True, ESWD=True):\n",
    "    \n",
    "    ds=str(day)\n",
    "    \n",
    "    #Choose starting day to make selection (1 day ahead of this date)\n",
    "    dp=datetime.datetime.strptime(ds,'%Y%j')\n",
    "    dstr=dp.isoformat()[0:10] #datetime converted in string\n",
    "\n",
    "    #parameter to shrink all colorbars to same size\n",
    "    shrink_cb=0.8\n",
    "    pad_cb=-0.005\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (17,9))\n",
    "\n",
    "    if (x_lim != None) & (y_lim != None):     \n",
    "        plt.xlim(x_lim)\n",
    "        plt.ylim(y_lim)\n",
    "        \n",
    "    '''\n",
    "    PLOT shapefile\n",
    "    '''\n",
    "    sf_geo[0].plot(ax=ax, alpha = 0.5, facecolor = 'none', lw = 0.5, zorder=2)\n",
    "    if len(sf_geo) > 1:\n",
    "        sf_geo[1].plot(ax=ax, alpha = 0.5, facecolor = 'none', lw = 0.5, zorder=2)\n",
    "    \n",
    "    '''\n",
    "    PLOT Dtemp: infrared brightness - tropopause temperature, (gridded) with colorbar\n",
    "    '''\n",
    "    if OT == True:\n",
    "        cm=pl.pcolor(cpclon,cpclat,dtmin,cmap=\"gist_stern\", zorder=2, alpha=0.9)\n",
    "        pl.clim([-10,15])\n",
    "        cb_ot = pl.colorbar(shrink=shrink_cb, pad=pad_cb)\n",
    "        cb_ot.ax.tick_params(axis='both', which='major', labelsize=13)\n",
    "        cb_ot.ax.set_ylabel('Min (IR brightness - tropopause Temp) sat-observed [°C]', fontsize=15);\n",
    "   \n",
    "    '''\n",
    "    PLOT EUCLID lightning data (punctual)\n",
    "    '''\n",
    "    #Plot lightning data with colorbar varying on hour of the day\n",
    "    #find beginning and end of datetime of insurance claims\n",
    "    if EUCLID == True:\n",
    "\n",
    "        if len(df_lg_sel)>0:\n",
    "\n",
    "            beg_l = df_lg_sel.datetime.loc[df_lg_sel.index[0]]\n",
    "            end_l = df_lg_sel.datetime.iloc[-1]\n",
    "            \n",
    "            tdelta_l = pd.Series(end_l-beg_l).dt.round(df_lg_trange)  #how many hours of reports, depending on df_lg_trange input\n",
    "        \n",
    "            if df_lg_trange == '60min':   #'H'\n",
    "                df_lg_tindex = 1\n",
    "            elif df_lg_trange == '30min':\n",
    "                df_lg_tindex = 2\n",
    "\n",
    "            #list of dates to be used in colorbar: one every hour starting from the first value and arriving to end:\n",
    "            date_list_l = pd.Series([beg_l])\n",
    "            for i in np.arange(1,int(tdelta_l.astype('timedelta64[h]')+1)*df_lg_tindex):\n",
    "                date_list_l = date_list_l.append(pd.Series([beg_l + timedelta(minutes=int(i)*60/df_lg_tindex)]))\n",
    "            date_list_l = date_list_l.reset_index(drop=True)\n",
    "\n",
    "            #set extreme values of colorbar\n",
    "            num_date_list_l = pd.to_numeric(date_list_l)\n",
    "            vmin_l=pd.to_numeric(num_date_list_l[0])\n",
    "            vmax_l=pd.to_numeric(num_date_list_l[len(date_list_l) - 1])\n",
    "            tdelta_cb_l = int(pd.to_numeric(pd.Series(timedelta(minutes=int(60/df_lg_tindex)))))\n",
    "            \n",
    "            cmap_l = plt.cm.spring\n",
    "            norm_l =matplotlib.colors.BoundaryNorm(np.arange(vmin_l, vmax_l+tdelta_cb_l, tdelta_cb_l), cmap_l.N)\n",
    "\n",
    "            #plot locations of lightnings\n",
    "            c5=pl.scatter(df_lg_sel.lon, df_lg_sel.lat,10,c=df_lg_sel['datetime'],cmap=cmap_l,marker=\".\",\n",
    "                          alpha=0.8, label='Lightnings',norm=norm_l, zorder=3)  \n",
    "\n",
    "            #add colorbar\n",
    "            cb_l = pl.colorbar(c5, orientation='vertical',norm=norm_l, shrink=shrink_cb, pad=pad_cb)\n",
    "            cb_l.ax.get_yaxis().set_ticks(pd.to_numeric(date_list_l))\n",
    "            cb_l.ax.set_yticklabels(date_list_l.dt.strftime('%H.%M')[:], fontsize=13)\n",
    "            cb_l.ax.set_ylabel('EUCLID lightning strikes timing (UTC)', fontsize=15)\n",
    "\n",
    "    '''\n",
    "    PLOT insurance/ESWD data (punctual)\n",
    "    '''\n",
    "    if ESWD == True:\n",
    "        \n",
    "        #Plot insurance/eswd data with colorbar varying on hour of the day\n",
    "        \n",
    "        #find beginning and end of datetime of insurance claims\n",
    "        if len(u_ev_sel)>0:\n",
    "            dt_dataset = u_ev_sel\n",
    "        else:\n",
    "            dt_dataset = eswd_ev_sel\n",
    "        \n",
    "        #divide eswd dataset in datasets with info on size of hail or not:\n",
    "        if len(eswd_ev_sel)>0:    \n",
    "            eswd_ev_sel_NOsize = eswd_ev_sel.loc[np.isnan(eswd_ev_sel['size']) == True].reset_index(drop=True)\n",
    "            eswd_ev_sel_wsize = eswd_ev_sel.loc[np.isnan(eswd_ev_sel['size']) == False].reset_index(drop=True)\n",
    "        \n",
    "        cmap = plt.cm.autumn\n",
    "        \n",
    "        #Conditions: if all eswd/ins data are relative to the same datetime (including also the case of 1 only data),\n",
    "        #or if all data have datetimes differing only of less than eswd_trange (e.g. 30 minutes) -> colorcode with\n",
    "        #only one color, otherwise create colorbar:\n",
    "        \n",
    "        if (all(i == dt_dataset.datetime[0] for i in dt_dataset.datetime) == False):  \n",
    "        \n",
    "            beg = dt_dataset.datetime.loc[dt_dataset.index[0]]\n",
    "            end = dt_dataset.datetime.iloc[-1]\n",
    "\n",
    "            tdelta = pd.Series(end-beg).dt.round(eswd_trange)  #how many hours of reports, depending on eswd_trange input\n",
    "\n",
    "            if eswd_trange == '60min':   #'H'\n",
    "                eswd_tindex = 1\n",
    "            elif eswd_trange == '30min':\n",
    "                eswd_tindex = 2\n",
    "            \n",
    "            if pd.to_timedelta(tdelta.values) > pd.to_timedelta(eswd_trange):\n",
    "            \n",
    "                cbar_plot = True\n",
    "                #list of dates to be used in colorbar: one every eswd_trange starting from the first value and \n",
    "                #arriving to end:\n",
    "                date_list = pd.Series([beg])\n",
    "                for i in np.arange(1,int(tdelta.astype('timedelta64[h]')+1)*eswd_tindex):\n",
    "                    date_list = date_list.append(pd.Series([beg + timedelta(minutes=int(i)*60/eswd_tindex)]))\n",
    "                date_list = date_list.reset_index(drop=True)\n",
    "\n",
    "                #set extreme values of colorbar\n",
    "                num_date_list = pd.to_numeric(date_list)\n",
    "                vmin=pd.to_numeric(num_date_list[0])\n",
    "                vmax=pd.to_numeric(num_date_list[len(date_list) - 1])\n",
    "                tdelta_cb = int(pd.to_numeric(pd.Series(timedelta(minutes=int(60/eswd_tindex)))))\n",
    "\n",
    "                norm = matplotlib.colors.BoundaryNorm(np.arange(vmin, vmax+tdelta_cb, tdelta_cb), cmap.N)\n",
    "                \n",
    "                if len(u_ev_sel)>0:\n",
    "                    col_code_u = u_ev_sel['datetime']\n",
    "                \n",
    "                if len(eswd_ev_sel)>0:    \n",
    "                    col_code_e_NOsize = eswd_ev_sel_NOsize['datetime']\n",
    "                    col_code_e_wsize = eswd_ev_sel_wsize['datetime']\n",
    "                    \n",
    "            else:\n",
    "                cbar_plot = False\n",
    "                norm = None\n",
    "                \n",
    "                if len(u_ev_sel)>0:\n",
    "                    col_code_u = 'red'\n",
    "                \n",
    "                if len(eswd_ev_sel)>0:    \n",
    "                    col_code_e_NOsize = 'red'\n",
    "                    col_code_e_wsize = 'red'\n",
    "                \n",
    "        else:\n",
    "            cbar_plot = False\n",
    "            norm = None\n",
    "                \n",
    "            if len(u_ev_sel)>0:\n",
    "                col_code_u = 'red'\n",
    "\n",
    "            if len(eswd_ev_sel)>0:    \n",
    "                col_code_e_NOsize = 'red'\n",
    "                col_code_e_wsize = 'red'  \n",
    "        \n",
    "        #plot locations of UNIPOL reports\n",
    "        if len(u_ev_sel)>0:\n",
    "            c4=pl.scatter(u_ev_sel.lon, u_ev_sel.lat,60,c=col_code_u,cmap=cmap,marker=\"o\",edgecolor='k', \n",
    "                          alpha=0.8, label='Insurance claims',norm=norm, zorder=4)    \n",
    "\n",
    "        #plot locations of ESWD hail reports with symbol proportional to the size of reported hail (or with different color)\n",
    "        if len(eswd_ev_sel)>0:\n",
    "            \n",
    "            if len(eswd_ev_sel_NOsize) > 0:\n",
    "                    c3_1=pl.scatter(eswd_ev_sel_NOsize.lon,eswd_ev_sel_NOsize.lat,35,c=col_code_e_NOsize,\n",
    "                            cmap=cmap,norm=norm,marker=\"v\",edgecolor='k',alpha=0.9,zorder=4)\n",
    "            if len(eswd_ev_sel_wsize) > 0:\n",
    "                    c3_2=pl.scatter(eswd_ev_sel_wsize.lon,eswd_ev_sel_wsize.lat,25*eswd_ev_sel_wsize['size'],\n",
    "                                    c=col_code_e_wsize,cmap=cmap,norm=norm,marker=\"^\",\n",
    "                                    edgecolor='k',alpha=0.9,zorder=4)\n",
    "        \n",
    "        #add colorbar (depending on eswd or unipol datasets)\n",
    "        if cbar_plot == True:\n",
    "            if len(eswd_ev_sel) > len(u_ev_sel):\n",
    "                c_bar = c3_2\n",
    "            else:\n",
    "                c_bar = c4\n",
    "\n",
    "            cb = pl.colorbar(c_bar, orientation='vertical',norm=norm, shrink=shrink_cb, pad=0.025)\n",
    "            cb.ax.get_yaxis().set_ticks(pd.to_numeric(date_list))\n",
    "            cb.ax.set_yticklabels(date_list.dt.strftime('%H.%M')[:], fontsize=13)\n",
    "            cb.ax.set_ylabel('Hail report timing (UTC)', fontsize=15)\n",
    "\n",
    "        unip = mlines.Line2D([], [], color='orange', marker='o', markersize=8, markeredgecolor='k', ls='', \n",
    "                             label='Insurance claims')\n",
    "        eswd_1 = mlines.Line2D([], [], color='orange', marker='^', markersize=11, markeredgecolor='k', ls='', \n",
    "                     label='ESWD reports \\n prop. to hail size')\n",
    "        eswd_2 = mlines.Line2D([], [], color='orange', marker='v', markersize=11, markeredgecolor='k', ls='', \n",
    "                             label='ESWD reports \\n no info on hail size')\n",
    "        pl.legend(handles=[unip, eswd_1, eswd_2],loc='upper right', fontsize=12);\n",
    "\n",
    "    #Plot locations and labels of selected cities\n",
    "    for k in range(len(clonsel)):  \n",
    "        pl.plot(clonsel[k],clatsel[k],'ko',markersize=6)\n",
    "        pl.plot(clonsel[k],clatsel[k],'wo',markersize=5)\n",
    "        pl.text(clonsel[k]+.055,clatsel[k]-.055, cnamesel[k],color='w',size=14)\n",
    "        pl.text(clonsel[k]+.05,clatsel[k]-.05, cnamesel[k],size=14)\n",
    "\n",
    "    pl.grid(ls=(0,(5,5)))\n",
    "    pl.title('Event '+str(dstr), fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5239158",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function for plotting hourly SPHERA data\n",
    "\"\"\"\n",
    "\n",
    "def HF_reanProxies_plot(sf_geo, sf_sp, x_lim, y_lim, df_sp, sp_hour, sp_par, clonsel, clatsel, cnamesel, hourly=True):\n",
    "    \n",
    "    #condition to select in GeoDataframe hourly values if output wanted in hourly, or daily-aggregated:\n",
    "    if hourly == True:\n",
    "        #select hour of the day for the plot\n",
    "        df_sp_Hsel = df_sp.loc[df_sp.datetime.apply(lambda x: x.hour) == sp_hour].reset_index(drop=True)\n",
    "        \n",
    "        #def. geodataframe with box_id, parameter and geometry of sf_sp to plot data on gridded map:\n",
    "        #condition on geometry because box_id != index (the upper right corner of SPHERA domain is not covered due \n",
    "        #to grid rotation, so some cells must be excluded otherwise data are displaced in the wrong boxes!)\n",
    "        gdf_sp_sel = gpd.GeoDataFrame(df_sp_Hsel,\n",
    "                                      geometry=sf_sp.loc[sf_sp.index[list(df_sp_Hsel['box_id'])]].reset_index(drop=True)['geometry'])\n",
    "        \n",
    "    else:\n",
    "        #def. geodataframe with box_id, parameter and geometry of sf_sp to plot data on gridded map:\n",
    "        gdf_sp_sel = gpd.GeoDataFrame(df_sp[[\"box_id\",f\"{sp_par}\"]], geometry=sf_sp['geometry'])\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize = (12,8))\n",
    "\n",
    "    #maps limits\n",
    "    if (x_lim != None) & (y_lim != None):     \n",
    "        plt.xlim(x_lim)\n",
    "        plt.ylim(y_lim)\n",
    "        \n",
    "    #grid shapefile\n",
    "    sf_sp.plot(ax=ax, alpha = 0.5, facecolor = 'none', lw = 0.3, zorder=2)\n",
    "\n",
    "    #geographic shapefiles\n",
    "    sf_geo[0].plot(ax=ax, alpha = 0.5, facecolor = 'none', lw = 0.5, zorder=2)  #ita shapefile\n",
    "    if len(sf_geo) > 1:\n",
    "        sf_geo[1].plot(ax=ax, alpha = 0.5, facecolor = 'none', lw = 0.5, zorder=2)  #other shapefile\n",
    "    \n",
    "    #color-code grid cells for a certain parameter:\n",
    "    if sp_par == 'LI':\n",
    "        col_num = 3\n",
    "        un_meas = '[°C]'  #unit measure for colorbar label\n",
    "        cbar_ext = 'max' #extension of the colorbar over defined ranges\n",
    "    elif sp_par == 'Kindex':\n",
    "        col_num = 4\n",
    "        cbar_ext = 'both'\n",
    "        un_meas = '[°C]'\n",
    "    elif sp_par == '%VV700':\n",
    "        col_num = 5\n",
    "        cbar_ext = 'neither'\n",
    "        un_meas = '%'\n",
    "    elif (sp_par == 'CAPE_MU') or (sp_par == 'CAPE_ML'):\n",
    "        col_num = 6\n",
    "        cbar_ext = 'max'\n",
    "        un_meas = '[J/kg]'\n",
    "    elif sp_par == 'H0':\n",
    "        col_num = 7\n",
    "        cbar_ext = 'both'\n",
    "        un_meas = '[m]'\n",
    "    elif sp_par == 'DLS':\n",
    "        col_num = 8\n",
    "        cbar_ext = 'both'\n",
    "        un_meas = '[m/s]'\n",
    "        \n",
    "    #calculate colorcoding dependent from the chosen parameter\n",
    "    col_boxes = gdf_sp_sel.box_id    \n",
    "    color_ton, bins, colors = HF_calc_color(gdf_sp_sel[sp_par], color=col_num)\n",
    "    \n",
    "    #plot geodataframe of SPHERA data with color_ton as colorcoding\n",
    "    gdf_sp_sel.plot(ax=ax, color=color_ton)\n",
    "\n",
    "    #add colorbar\n",
    "    cmap_sp_par = matplotlib.colors.ListedColormap(sns.color_palette(colors).as_hex())\n",
    "    norm = matplotlib.colors.BoundaryNorm(bins, cmap_sp_par.N, extend=cbar_ext)\n",
    "\n",
    "    img = plt.imshow([bins], cmap=cmap_sp_par, norm=norm)\n",
    "    img.set_visible(False)\n",
    "\n",
    "    cb_sp_par=plt.colorbar(orientation='vertical', spacing='proportional', norm=norm,  pad=0.025, shrink=0.8);\n",
    "    cb_sp_par.ax.set_yticklabels(bins, fontsize=13)\n",
    "    cb_sp_par.ax.set_ylabel(f'{sp_par}  {un_meas}', fontsize=15);\n",
    "    \n",
    "    \n",
    "    #Plot locations and labels of selected cities\n",
    "    for k in range(len(clonsel)):  \n",
    "        pl.plot(clonsel[k],clatsel[k],'ko',markersize=6)\n",
    "        pl.plot(clonsel[k],clatsel[k],'wo',markersize=5)\n",
    "        pl.text(clonsel[k]+.055,clatsel[k]-.055, cnamesel[k],color='w',size=14)\n",
    "        pl.text(clonsel[k]+.05,clatsel[k]-.05, cnamesel[k],size=14)\n",
    "    \n",
    "    if hourly == True:\n",
    "        title = str(gdf_sp_sel.datetime[0]) + \" Param: \" + sp_par\n",
    "    else:\n",
    "        title = 'Daily aggregation' + \" Param: \" + sp_par\n",
    "    \n",
    "    #pl.grid(ls=(0,(5,5)))\n",
    "    pl.title(label=title, fontsize=15);    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81b91e4",
   "metadata": {},
   "source": [
    "## Functions for getting ESWD-based SPHERA reanalysis distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d13166",
   "metadata": {},
   "source": [
    "PREVIOUS VERSION WITH 4 HOURS\n",
    "\n",
    "\"\"\"\n",
    "Function for extracting the temporal aggregations of SPHERA params as GeoDataframe over the last 4h before ESWD rep\n",
    "Inputs: gdf_eswd_1rep = 1eswd report in geodataframe row, sp_eswd = dict of sphera dataframes, sf_sp = sphera shapef\n",
    "\n",
    "Possible problem of this function: if eswd original hour is >=23.30 -> rounded to 00 and the function will take the\n",
    "hours one day before (only 2 cases like this in the dataset i have: 2018-06-04 23:40:00 and 2018-06-04 23:44:00\n",
    "treated singularly!\n",
    "\"\"\"\n",
    "\n",
    "def HF_rean_tAgg_ESWDh(gdf_eswd_1rep, sp_eswd, sf_sp):\n",
    "\n",
    "    #select dataframe of SPHERA with the same day of the eswd event (and of the day before if needed):\n",
    "    df_sp_eswd =  sp_eswd[gdf_eswd_1rep.datetime.date()]\n",
    "\n",
    "    if gdf_eswd_1rep.datetime.date()-timedelta(days=1) in list(sp_eswd.keys()):\n",
    "        \n",
    "        df_sp_eswd_Dbefore = sp_eswd[gdf_eswd_1rep.datetime.date()-timedelta(days=1)]\n",
    "\n",
    "    #find timing of ESWD occurrence (round to hour every 30 mins)\n",
    "    eswd_hour = (gdf_eswd_1rep.datetime.replace(second=0, microsecond=0, minute=0, hour=gdf_eswd_1rep.datetime.hour) \n",
    "                 +timedelta(hours=gdf_eswd_1rep.datetime.minute//30)).hour\n",
    "\n",
    "    #condition if all the 4hours fall within the OT day of the event:\n",
    "    if eswd_hour >= 3:\n",
    "\n",
    "        #select the last 4 hours from SPHERA daily dataset\n",
    "        df_sp_4ESWDh = df_sp_eswd.loc[(df_sp_eswd.datetime.dt.hour > (int(eswd_hour) - 4)) & \n",
    "                                      (df_sp_eswd.datetime.dt.hour <= int(eswd_hour))].reset_index(drop=True)\n",
    "\n",
    "    #condition if the 4hours are straddling 2 days:\n",
    "    elif eswd_hour <=2:\n",
    "        df_sp_4ESWDh = df_sp_eswd.loc[(df_sp_eswd.datetime.dt.hour > (int(eswd_hour) - 4)) & \n",
    "                               (df_sp_eswd.datetime.dt.hour <= int(eswd_hour))].reset_index(drop=True)\n",
    "\n",
    "        df_sp_4ESWDh_Dbefore = df_sp_eswd_Dbefore.loc[(df_sp_eswd_Dbefore.datetime.dt.hour \n",
    "                                                       >= 21+eswd_hour)].reset_index(drop=True)\n",
    "\n",
    "        df_sp_4ESWDh = pd.concat([df_sp_4ESWDh_Dbefore,df_sp_4ESWDh]).reset_index(drop=True)\n",
    "\n",
    "    #build sphera geodataframe containing temporal aggregation over the last 4 hours (max/min depending on param):\n",
    "    gdf_sp_4ESWDh = gpd.GeoDataFrame(columns=['datetime_agg','box_id','%VV700','Kindex','LI','DLS','H0',\n",
    "                                           'CAPE_MU','CAPE_ML','geometry'])\n",
    "\n",
    "    gdf_sp_4ESWDh['box_id'] = df_sp_4ESWDh.groupby(['box_id'], as_index=False).max()['box_id']\n",
    "    gdf_sp_4ESWDh['datetime_agg'][:] = df_sp_4ESWDh[\"datetime\"].iloc[-1]  #referring to last hour of aggregation\n",
    "    gdf_sp_4ESWDh['%VV700'] = df_sp_4ESWDh.groupby(['box_id'], as_index=False).max()['%VV700']\n",
    "    gdf_sp_4ESWDh['Kindex'] = df_sp_4ESWDh.groupby(['box_id'], as_index=False).max()['Kindex']\n",
    "    gdf_sp_4ESWDh['LI'] = df_sp_4ESWDh.groupby(['box_id'], as_index=False).min()['LI']\n",
    "    gdf_sp_4ESWDh['DLS'] = df_sp_4ESWDh.groupby(['box_id'], as_index=False).max()['DLS']\n",
    "    gdf_sp_4ESWDh['H0'] = df_sp_4ESWDh.groupby(['box_id'], as_index=False).min()['H0']\n",
    "    gdf_sp_4ESWDh['CAPE_MU'] = df_sp_4ESWDh.groupby(['box_id'], as_index=False).max()['CAPE_MU']\n",
    "    gdf_sp_4ESWDh['CAPE_ML'] = df_sp_4ESWDh.groupby(['box_id'], as_index=False).max()['CAPE_ML']\n",
    "    gdf_sp_4ESWDh['geometry'] = sf_sp.loc[sf_sp.index[list(gdf_sp_4ESWDh['box_id'])]].reset_index(drop=True)['geometry']\n",
    "\n",
    "    return gdf_sp_4ESWDh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8c07561",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function for extracting the temporal aggregations of SPHERA params as GeoDataframe over the last 4h before ESWD rep\n",
    "Inputs: gdf_eswd_1rep = 1eswd report in geodataframe row, sp_eswd = dict of sphera dataframes, sf_sp = sphera shapef\n",
    "\n",
    "Possible problem of this function: if eswd original hour is >=23.30 -> rounded to 00 and the function will take the\n",
    "hours one day before (only 2 cases like this in the dataset i have: 2018-06-04 23:40:00 and 2018-06-04 23:44:00\n",
    "treated singularly!\n",
    "\"\"\"\n",
    "\n",
    "def HF_rean_tAgg_ESWDh(gdf_eswd_1rep, sp_eswd, sf_sp):\n",
    "\n",
    "    #select dataframe of SPHERA with the same day of the eswd event (and of the day before if needed):\n",
    "    df_sp_eswd =  sp_eswd[gdf_eswd_1rep.datetime.date()]\n",
    "\n",
    "    if gdf_eswd_1rep.datetime.date()-timedelta(days=1) in list(sp_eswd.keys()):\n",
    "        \n",
    "        df_sp_eswd_Dbefore = sp_eswd[gdf_eswd_1rep.datetime.date()-timedelta(days=1)]\n",
    "\n",
    "    #find timing of ESWD occurrence (round to hour every 30 mins)\n",
    "    eswd_hour = (gdf_eswd_1rep.datetime.replace(second=0, microsecond=0, minute=0, hour=gdf_eswd_1rep.datetime.hour) \n",
    "                 +timedelta(hours=gdf_eswd_1rep.datetime.minute//30)).hour\n",
    "\n",
    "    #condition if all the 3hours fall within the OT day of the event:\n",
    "    if eswd_hour >= 2:\n",
    "\n",
    "        #select the last 3 hours from SPHERA daily dataset\n",
    "        df_sp_3ESWDh = df_sp_eswd.loc[(df_sp_eswd.datetime.dt.hour > (int(eswd_hour) - 3)) & \n",
    "                                      (df_sp_eswd.datetime.dt.hour <= int(eswd_hour))].reset_index(drop=True)\n",
    "\n",
    "    #condition if the 3hours are straddling 2 days:\n",
    "    elif eswd_hour <=1:\n",
    "        df_sp_3ESWDh = df_sp_eswd.loc[(df_sp_eswd.datetime.dt.hour > (int(eswd_hour) - 3)) & \n",
    "                               (df_sp_eswd.datetime.dt.hour <= int(eswd_hour))].reset_index(drop=True)\n",
    "\n",
    "        df_sp_3ESWDh_Dbefore = df_sp_eswd_Dbefore.loc[(df_sp_eswd_Dbefore.datetime.dt.hour \n",
    "                                                       >= 22+eswd_hour)].reset_index(drop=True)\n",
    "\n",
    "        df_sp_3ESWDh = pd.concat([df_sp_3ESWDh_Dbefore,df_sp_3ESWDh]).reset_index(drop=True)\n",
    "   \n",
    "    #build sphera geodataframe containing temporal aggregation over the last 3 hours (max/min depending on param):\n",
    "    gdf_sp_3ESWDh = gpd.GeoDataFrame(columns=['datetime_agg','box_id','%VV700','Kindex','LI','DLS','H0',\n",
    "                                           'CAPE_MU','CAPE_ML','geometry'])\n",
    "\n",
    "    gdf_sp_3ESWDh['box_id'] = df_sp_3ESWDh.groupby(['box_id'], as_index=False).max()['box_id']\n",
    "    gdf_sp_3ESWDh['datetime_agg'][:] = df_sp_3ESWDh[\"datetime\"].iloc[-1]  #referring to last hour of aggregation\n",
    "    gdf_sp_3ESWDh['%VV700'] = df_sp_3ESWDh.groupby(['box_id'], as_index=False).max()['%VV700']\n",
    "    gdf_sp_3ESWDh['Kindex'] = df_sp_3ESWDh.groupby(['box_id'], as_index=False).max()['Kindex']\n",
    "    gdf_sp_3ESWDh['LI'] = df_sp_3ESWDh.groupby(['box_id'], as_index=False).min()['LI']\n",
    "    gdf_sp_3ESWDh['DLS'] = df_sp_3ESWDh.groupby(['box_id'], as_index=False).max()['DLS']\n",
    "    gdf_sp_3ESWDh['H0'] = df_sp_3ESWDh.groupby(['box_id'], as_index=False).min()['H0']\n",
    "    gdf_sp_3ESWDh['CAPE_MU'] = df_sp_3ESWDh.groupby(['box_id'], as_index=False).max()['CAPE_MU']\n",
    "    gdf_sp_3ESWDh['CAPE_ML'] = df_sp_3ESWDh.groupby(['box_id'], as_index=False).max()['CAPE_ML']\n",
    "    gdf_sp_3ESWDh['geometry'] = sf_sp.loc[sf_sp.index[list(gdf_sp_3ESWDh['box_id'])]].reset_index(drop=True)['geometry']\n",
    "\n",
    "    return gdf_sp_3ESWDh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48367d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function for extracting the spatial windows around every SPHERA cell containing ESWD reports, write a\n",
    "geodataframe containing all the cells in the neighbourhood of ESWD detection\n",
    "\"\"\"\n",
    "\n",
    "def HF_rean_spatWindowESWD(gdf_eswd_1rep, dgdf_sp_3ESWDh_1rep):\n",
    "\n",
    "    #condition if lat/lon has only 1 digit after comma (i.e. they are in the border of a SPHERA grid cell):\n",
    "    if len(repr(gdf_eswd_1rep.lon).split('.')[1]) == 1:\n",
    "        gdf_eswd_1rep['lon'] = gdf_eswd_1rep['lon'] - 0.000001\n",
    "    if len(repr(gdf_eswd_1rep.lat).split('.')[1]) == 1:\n",
    "         gdf_eswd_1rep['lat'] = gdf_eswd_1rep['lat'] - 0.000001\n",
    "\n",
    "    gdf_eswd_1rep['geometry'] = gpd.points_from_xy([gdf_eswd_1rep.lon], [gdf_eswd_1rep.lat])\n",
    "\n",
    "    #extract geometry of eswd report loc:\n",
    "    ESWDpoint = gdf_eswd_1rep.geometry\n",
    "\n",
    "    #loop to find the sphera cell in which eswd report falls:\n",
    "    for s_cell in dgdf_sp_3ESWDh_1rep.index:    \n",
    "\n",
    "        if ESWDpoint.within(dgdf_sp_3ESWDh_1rep['geometry'][s_cell]):       \n",
    "\n",
    "            S_ESWDcell = dgdf_sp_3ESWDh_1rep.loc[s_cell]\n",
    "\n",
    "    #Select for the S_ESWDcell the spatial neighbourhood of 7x7=49 grid cells around it (res. of approx 0.63° 70km):\n",
    "    #select 70km-nearest neighbourhood (nn) around the cell (the 48+1(itself) grid  cells having the smallest dist.):\n",
    "    nn_ind = dgdf_sp_3ESWDh_1rep.geometry.distance(S_ESWDcell.geometry).sort_values()[:49].index\n",
    "    gdf_sp_3ESWDh_nn = dgdf_sp_3ESWDh_1rep.loc[nn_ind]\n",
    "    \n",
    "    return gdf_sp_3ESWDh_nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc189c06",
   "metadata": {},
   "source": [
    "## Functions for OT-filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "530c26f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function for reading processed hourly OT data, corrected for Parallax, and write them as GeoDataFrames\n",
    "\"\"\"\n",
    "\n",
    "def HF_OTdata_2_gdf(cpclat,cpclon,otpmax,ot_timing):\n",
    "    \n",
    "    #Read multi-dim xarray containing three arrays otpmax, cpclat and cpclon (already corrected for parallax factor!)\n",
    "    #WITH DATASET: BETTER\n",
    "    xr_OT = xr.Dataset(data_vars=None, \n",
    "                       coords=dict(\n",
    "                           lon=([\"xlon\", \"xlat\"], cpclon),\n",
    "                           lat=([\"xlon\", \"xlat\"], cpclat),\n",
    "                           time=ot_timing[0],\n",
    "                       ),\n",
    "                       attrs=dict(\n",
    "                       description=\"OT max probability over SPHERA domain accumulated over the hour indicated in time\",\n",
    "                       units=\"%\",\n",
    "                   ),\n",
    "    )\n",
    "    \n",
    "    #set attributes of xr dataset:\n",
    "    xr_OT[\"lon\"].attrs[\"long_name\"] = \"longitude\"\n",
    "    xr_OT[\"lon\"].attrs[\"units\"] = \"degree_east\"\n",
    "    xr_OT[\"lon\"].attrs[\"axis\"] = \"X\"\n",
    "    xr_OT[\"lat\"].attrs[\"long_name\"] = \"latitude\"\n",
    "    xr_OT[\"lat\"].attrs[\"units\"] = \"degree_north\"\n",
    "    xr_OT[\"lat\"].attrs[\"axis\"] = \"Y\"\n",
    "\n",
    "    #Trick: set otpmax variable starting from lat array:\n",
    "    xr_OT['otpmax'] = xr_OT.lat*np.nan\n",
    "    xr_OT.otpmax.values = otpmax\n",
    "    \n",
    "    #convert xarray dataset to pandas dataframe\n",
    "    df_xr_OT = xr_OT.to_dataframe()\n",
    "\n",
    "    #convert dataframe to Geodataframe setting up geometry made by lat/lon points:\n",
    "    gdf_xr_OT = gpd.GeoDataFrame(df_xr_OT, geometry=gpd.points_from_xy(df_xr_OT.lon, df_xr_OT.lat))\n",
    "\n",
    "    #extract only the subdataframe containing non null values of otpmax:\n",
    "    gdf_xr_OT_nn = gdf_xr_OT[~gdf_xr_OT['otpmax'].isnull() == True]\n",
    "    \n",
    "    return gdf_xr_OT_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb1d212b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function for extracting the temporal aggregations of SPHERA params as GeoDataframe over the last 4h before OT\n",
    "\"\"\"\n",
    "\n",
    "def HF_rean_tAgg_OTh(gdf_OT, df_sp, df_sp_Dbefore, sf_sp):\n",
    "    \n",
    "    #find timing of OT occurrence\n",
    "    OT_hour = int(pd.to_datetime(gdf_OT.time[:1]).reset_index(drop=True).dt.hour)\n",
    "    \n",
    "    #condition if all the 3 hours fall within the OT day of the event:\n",
    "    if OT_hour >= 2:\n",
    "\n",
    "        #select the last 3 hours from SPHERA daily dataset\n",
    "        df_sp_3OTh = df_sp.loc[(df_sp.datetime.dt.hour > (int(OT_hour) - 3)) & \n",
    "                               (df_sp.datetime.dt.hour <= int(OT_hour))].reset_index(drop=True)\n",
    "\n",
    "    #condition if the 3 hours are straddling 2 days:\n",
    "    elif OT_hour <=1:\n",
    "        df_sp_3OTh = df_sp.loc[(df_sp.datetime.dt.hour > (int(OT_hour) - 3)) & \n",
    "                               (df_sp.datetime.dt.hour <= int(OT_hour))].reset_index(drop=True)\n",
    "\n",
    "        df_sp_3OTh_Dbefore = df_sp_Dbefore.loc[(df_sp_Dbefore.datetime.dt.hour >= 22+OT_hour)].reset_index(drop=True)\n",
    "\n",
    "        df_sp_3OTh = pd.concat([df_sp_3OTh_Dbefore,df_sp_3OTh]).reset_index(drop=True)\n",
    "    \n",
    "    #build sphera geodataframe containing temporal aggregation over the last 3 hours (max/min depending on param):\n",
    "    gdf_sp_3OTh = gpd.GeoDataFrame(columns=['datetime_agg','box_id','%VV700','Kindex','LI','DLS','H0',\n",
    "                                           'CAPE_MU','CAPE_ML','geometry'])\n",
    "\n",
    "    gdf_sp_3OTh['box_id'] = df_sp_3OTh.groupby(['box_id'], as_index=False).max()['box_id']\n",
    "    gdf_sp_3OTh['datetime_agg'][:] = df_sp_3OTh[\"datetime\"].iloc[-1]  #referring to last hour of aggregation\n",
    "    gdf_sp_3OTh['%VV700'] = df_sp_3OTh.groupby(['box_id'], as_index=False).max()['%VV700']\n",
    "    gdf_sp_3OTh['Kindex'] = df_sp_3OTh.groupby(['box_id'], as_index=False).max()['Kindex']\n",
    "    gdf_sp_3OTh['LI'] = df_sp_3OTh.groupby(['box_id'], as_index=False).min()['LI']\n",
    "    gdf_sp_3OTh['DLS'] = df_sp_3OTh.groupby(['box_id'], as_index=False).max()['DLS']\n",
    "    gdf_sp_3OTh['H0'] = df_sp_3OTh.groupby(['box_id'], as_index=False).min()['H0']\n",
    "    gdf_sp_3OTh['CAPE_MU'] = df_sp_3OTh.groupby(['box_id'], as_index=False).max()['CAPE_MU']\n",
    "    gdf_sp_3OTh['CAPE_ML'] = df_sp_3OTh.groupby(['box_id'], as_index=False).max()['CAPE_ML']\n",
    "    gdf_sp_3OTh['geometry'] = sf_sp.loc[sf_sp.index[list(gdf_sp_3OTh['box_id'])]].reset_index(drop=True)['geometry']\n",
    "    \n",
    "    return gdf_sp_3OTh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58c3195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function for extracting the spatial windows around every SPHERA cell containing at least 1 OT detection, write a\n",
    "dictionary of geodataframes containing all the cells in the neighbourhood of OT detections\n",
    "\"\"\"\n",
    "\n",
    "def HF_rean_spatWindowOT(gdf_OT, gdf_sp_3OTh):\n",
    "    \n",
    "    # list of SPHERA cells containing the OT detections:\n",
    "    S_OTcells = pd.DataFrame(columns=gdf_sp_3OTh.columns)\n",
    "\n",
    "    #loop to identify which SPHERA grid contains each OT point\n",
    "    for point in gdf_OT.reset_index(drop=True).geometry:\n",
    "\n",
    "        for s_cell in gdf_sp_3OTh.index:    \n",
    "\n",
    "            if point.within(gdf_sp_3OTh['geometry'][s_cell]):       \n",
    "                \n",
    "                S_OTcells.loc[s_cell] = gdf_sp_3OTh.loc[s_cell]\n",
    "                           \n",
    "    #Select for every S_OTcell the spatial neighbourhood of 7x7=49 grid cells around it (res. of approx 0.63° 70km):\n",
    "    #write sub-geodataframes in a dictionary\n",
    "    dgdf_sp_3OTh_nn = dict()\n",
    "\n",
    "    #Loop to apply it to every cell containing at least 1 OT detection:\n",
    "    for cel in S_OTcells.geometry:\n",
    "        #identify cell\n",
    "        sp_cel = gdf_sp_3OTh[gdf_sp_3OTh.geometry ==  cel]\n",
    "\n",
    "        #select 70km-nearest neighbourhood (nn) around the cell (the 48+1(itself) grid  cells having the smallest dist.):\n",
    "        nn_ind = gdf_sp_3OTh.geometry.distance(cel).sort_values()[:49].index\n",
    "        dgdf_sp_3OTh_nn[int(sp_cel.index.values)] = gdf_sp_3OTh.loc[nn_ind]\n",
    "        \n",
    "    return dgdf_sp_3OTh_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "774d0763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HF_OTfilter(gdf_OT, dgdf_sp_3OTh_nn, t_CAPE, t_K, t_LI, t_DLS, t_H0):\n",
    "    \n",
    "    #Copy of the full initial OT dataset from which to remove occurrences after reanalysis filtering:\n",
    "    FILT_gdf_OT = gdf_OT\n",
    "    toBeFiltered = pd.DataFrame()\n",
    "\n",
    "    #cycle over every SPHERA cell containing OTs:\n",
    "    for SP_cell in dgdf_sp_3OTh_nn.keys():\n",
    "\n",
    "        #center cell is the first of the dictionary of its neighbourhood\n",
    "        sp_center = dgdf_sp_3OTh_nn[SP_cell].loc[[SP_cell],:]['geometry'][SP_cell]\n",
    "\n",
    "        #subset of OT contained in the cell\n",
    "        #gdf_OT_inCell = gdf_OT.reset_index(drop=True)[gdf_OT.geometry.reset_index(drop=True).within(sp_center)]\n",
    "        gdf_OT_inCell = gdf_OT[gdf_OT.geometry.within(sp_center)]\n",
    "        \n",
    "        #select SPHERA spatial neighbourhood of the cell\n",
    "        gdf_sp_neighbour = dgdf_sp_3OTh_nn[SP_cell]\n",
    "\n",
    "        #CONDITIONS ON REANALYSIS DATASET:\n",
    "        #all conditions refers to the requirement that at least one of the 49 sphera grid cells must fullfil\n",
    "        #(separately for each parameter) and they must be all valid in the same time!\n",
    "        if (any(gdf_sp_neighbour['CAPE_MU'] > t_CAPE) == True and any(gdf_sp_neighbour['Kindex'] > t_K) == True and\n",
    "            any(gdf_sp_neighbour['LI'] < t_LI) == True and any(gdf_sp_neighbour['DLS'] > t_DLS) == True and\n",
    "            any(gdf_sp_neighbour['H0'] < t_H0) == True):\n",
    "\n",
    "            print(f'{len(gdf_OT_inCell)} OTs kept!')\n",
    "\n",
    "        else:\n",
    "            #drop out OTs from FILT_gdf_OT not matching conditions\n",
    "            print(f'Filtered {len(gdf_OT_inCell)} OTs')\n",
    "            \n",
    "            #condition on legth of FILT_gdf_OT (cause if there is only one value equal to the one to be removed it should\n",
    "            #be explicitly considered)\n",
    "            if len(FILT_gdf_OT) > 1:\n",
    "                \n",
    "                #store OTs to be filtered later\n",
    "                toBeFiltered = pd.concat([toBeFiltered,gdf_OT_inCell])\n",
    "               \n",
    "                #FILT_gdf_OT = FILT_gdf_OT.drop(FILT_gdf_OT.loc[gdf_OT_inCell.index].index)\n",
    "                #FILT_gdf_OT = FILT_gdf_OT.drop(FILT_gdf_OT[FILT_gdf_OT.geometry.isin(OT_point)].index)\n",
    "                #FILT_gdf_OT = FILT_gdf_OT.drop(FILT_gdf_OT[FILT_gdf_OT.geometry.isin(gpd.GeoSeries(geometry.Point(OT_point_x, OT_point_y)))].index)\n",
    "                #FILT_gdf_OT = FILT_gdf_OT.drop(FILT_gdf_OT[FILT_gdf_OT.geometry.isin(gdf_OT_inCell.geometry)].index)\n",
    "\n",
    "            #condition if there is only 1 point in the full OT detections set            \n",
    "            elif (len(FILT_gdf_OT) == 1 and \n",
    "                  any(FILT_gdf_OT.reset_index()[['geometry']] == gdf_OT_inCell.reset_index()[['geometry']])):\n",
    "                FILT_gdf_OT = gpd.GeoDataFrame()\n",
    "            \n",
    "    #filter detected OTs:\n",
    "    if len(toBeFiltered) != 0:\n",
    "        FILT_gdf_OT = FILT_gdf_OT.drop(FILT_gdf_OT.loc[toBeFiltered.index].index)\n",
    "        \n",
    "    return FILT_gdf_OT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f33b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
